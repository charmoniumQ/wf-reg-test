@misc{hettrick_softwaresavedsoftware_in_research_survey_2014_2018,
	title = {Softwaresaved/{Software}\_In\_Research\_Survey\_2014: {Software} {In} {Research} {Survey}},
	copyright = {Open Access},
	shorttitle = {Softwaresaved/{Software}\_In\_Research\_Survey\_2014},
	url = {https://zenodo.org/record/1183562},
	abstract = {This reproducible, Python-based re-analysis of the Software Sustainability Institute's 2014 research software survey. The original analysis was conducted in Excel, so this re-analysis was performed to improve the reproducibility of the results.},
	urldate = {2022-05-26},
	publisher = {Zenodo},
	author = {Hettrick, Simon},
	month = feb,
	year = {2018},
	doi = {10.5281/ZENODO.1183562},
	keywords = {research software engineering, internship-project},
}

@book{merton_sociology_1974,
	address = {Chicago},
	edition = {4. Dr.},
	title = {The sociology of science: theoretical and empirical investigations},
	isbn = {978-0-226-52092-6},
	shorttitle = {The sociology of science},
	language = {eng},
	publisher = {Univ. of Chicago Pr},
	author = {Merton, Robert K.},
	year = {1974},
	keywords = {internship-project, sociology},
}

@article{collberg_repeatability_2016,
	title = {Repeatability in computer systems research},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2812803},
	doi = {10.1145/2812803},
	abstract = {To encourage repeatable research, fund repeatability engineering and reward commitments to sharing research artifacts.},
	language = {en},
	number = {3},
	urldate = {2022-05-27},
	journal = {Communications of the ACM},
	author = {Collberg, Christian and Proebsting, Todd A.},
	month = feb,
	year = {2016},
	keywords = {research software engineering, internship-project},
	pages = {62--69},
	file = {2812803.pdf:/home/sam/Zotero/storage/JGDDR733/2812803.pdf:application/pdf},
}

@book{taylor_workflows_2014,
	edition = {2007th edition},
	title = {Workflows for e-{Science}: {Scientific} {Workflows} for {Grids}},
	isbn = {978-1-84996-619-1},
	shorttitle = {Workflows for e-{Science}},
	url = {https://link.springer.com/book/10.1007/978-1-84628-757-2},
	abstract = {Workflows for e-Science is divided into four parts, which represent four broad but distinct areas of scientific workflows. In the first part, Background, we introduce the concept of scientific workflows and set the scene by describing how they differ from their business workflow counterpart. In Part II, Application and User Perspective, we provide a number of scientific examples that currently use workflows for their e-Science experiments. In Workflow Representation and Common Structure (Part III), we describe core workflow themes, such as control flow or dataflow and the use of components or services. In this part, we also provide overviews for a number of common workflow languages, such as Petri Nets, the Business Process Execution Language (BPEL), and the Virtual Data Language (VDL), along with service interfaces. In Part IV, Frameworks and Tools, we take a look at many of the popular environments that are currently being used for e-Science applications by paying particular attention to their workflow capabilities. The following four sections describe the chapters in each part and therefore provide a comprehensive summary of the book as a whole.},
	language = {English},
	publisher = {Springer},
	editor = {Taylor, Ian J. and Deelman, Ewa and Gannon, Dennis B. and Shields, Matthew},
	month = mar,
	year = {2014},
	keywords = {workflow managers},
	file = {Snapshot:/home/sam/Zotero/storage/92UFJE7C/978-1-84628-757-2.html:text/html;Taylor et al. - Workflows for e-Science.pdf:/home/sam/Zotero/storage/9KLYDT65/Taylor et al. - Workflows for e-Science.pdf:application/pdf},
}

@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2022-09-14},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	note = {Publisher: Public Library of Science},
	pages = {e124},
	file = {Full Text PDF:/home/sam/Zotero/storage/YNN44PSX/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf;Snapshot:/home/sam/Zotero/storage/BKJXJ6QD/article.html:text/html},
}

@article{gil_examining_2007,
	title = {Examining the {Challenges} of {Scientific} {Workflows}},
	volume = {40},
	issn = {1558-0814},
	doi = {10.1109/MC.2007.421},
	abstract = {Workflows have emerged as a paradigm for representing and managing complex distributed computations and are used to accelerate the pace of scientific progress. A recent National Science Foundation workshop brought together domain, computer, and social scientists to discuss requirements of future scientific applications and the challenges they present to current workflow technologies.},
	number = {12},
	journal = {Computer},
	author = {Gil, Yolanda and Deelman, Ewa and Ellisman, Mark and Fahringer, Thomas and Fox, Geoffrey and Gannon, Dennis and Goble, Carole and Livny, Miron and Moreau, Luc and Myers, Jim},
	month = dec,
	year = {2007},
	note = {Conference Name: Computer
interest: 91},
	keywords = {workflow managers},
	pages = {24--32},
	file = {Accepted Version:/home/sam/Zotero/storage/SHKAM8S5/Gil et al. - 2007 - Examining the Challenges of Scientific Workflows.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sam/Zotero/storage/FPVVU7W6/4404805.html:text/html},
}

@inproceedings{henkel_shipwright_2021,
	title = {Shipwright: {A} {Human}-in-the-{Loop} {System} for {Dockerfile} {Repair}},
	shorttitle = {Shipwright},
	doi = {10.1109/ICSE43902.2021.00106},
	abstract = {Docker is a tool for lightweight OS-level virtualization. Docker images are created by performing a build, controlled by a source-level artifact called a Dockerfile. We studied Dockerfiles on GitHub, and-to our great surprise-found that over a quarter of the examined Dockerfiles failed to build (and thus to produce images). To address this problem, we propose SHIPWRIGHT, a human-in-the-loop system for finding repairs to broken Dockerfiles. SHIPWRIGHT uses a modified version of the BERT language model to embed build logs and to cluster broken Dockerfiles. Using these clusters and a search-based procedure, we were able to design 13 rules for making automated repairs to Dockerfiles. With the aid of SHIPWRIGHT, we submitted 45 pull requests (with a 42.2\% acceptance rate) to GitHub projects with broken Dockerfiles. Furthermore, in a "time-travel" analysis of broken Dockerfiles that were later fixed, we found that SHIPWRIGHT proposed repairs that were equivalent to human-authored patches in 22.77\% of the cases we studied. Finally, we compared our work with recent, state-of-the-art, static Dockerfile analyses, and found that, while static tools detected possible build-failure-inducing issues in 20.6-33.8\% of the files we examined, SHIPWRIGHT was able to detect possible issues in 73.25\% of the files and, additionally, provide automated repairs for 18.9\% of the files.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Henkel, Jordan and Silva, Denini and Teixeira, Leopoldo and d’Amorim, Marcelo and Reps, Thomas},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {reproducibility engineering},
	pages = {1148--1160},
	file = {IEEE Xplore Full Text PDF:/home/sam/Zotero/storage/ME8QXGUQ/Henkel et al. - 2021 - Shipwright A Human-in-the-Loop System for Dockerf.pdf:application/pdf},
}

@inproceedings{zhao_why_2012,
	title = {Why workflows break — understanding and combating decay in {Taverna} workflows},
	url = {https://www.research.manchester.ac.uk/portal/en/publications/why-workflows-break--understanding-and-combating-decay-in-taverna-workflows(cba81ca4-e92c-408e-8442-383d1f15fcdf)/export.html},
	doi = {10.1109/eScience.2012.6404482},
	abstract = {Workflows provide a popular means for preserving scientific methods by explicitly encoding their process. However, some of them are subject to a decay in their ability to be re-executed or reproduce the same results over time, largely due to the volatility of the resources required for workflow executions. This paper provides an analysis of the root causes of workflow decay based on an empirical study of a collection of Taverna workflows from the myExperiment repository. Although our analysis was based on a specific type of workflow, the outcomes and methodology should be applicable to workflows from other systems, at least those whose executions also rely largely on accessing third-party resources. Based on our understanding about decay we recommend a minimal set of auxiliary resources to be preserved together with the workflows as an aggregation object and provide a software tool for end-users to create such aggregations and to assess their completeness},
	booktitle = {2012 {IEEE} 8th {International} {Conference} on {E}-{Science} (e-{Science})},
	author = {Zhao, Jun and Gomez-Perez, Jose-Manuel and Belhajjame, Khalid and Klyne, Graham and Garcia-cuesta, Esteban and Garrido, Aleix and Hettne, Kristina and Roos, Marco and De Roure, David and Goble, Carole},
	month = oct,
	year = {2012},
	note = {interest: 99},
	keywords = {workflow managers, internship-project},
	pages = {9},
	file = {Why_workflows_break__Understanding_and_combating_decay_in_Taverna_workflows.pdf:/home/sam/Zotero/storage/2BQSSKJF/Why_workflows_break__Understanding_and_combating_decay_in_Taverna_workflows.pdf:application/pdf},
}

@article{hinsen_dealing_2019,
	title = {Dealing {With} {Software} {Collapse}},
	volume = {21},
	issn = {1558-366X},
	doi = {10.1109/MCSE.2019.2900945},
	abstract = {Discusses the concept of software collapse. There is A good chance that you have never heard of software collapse before, for the simple reason that it is a term I have made up myself two years ago in a blog post. However, if you have been doing computational science for a few years, there is a good chance that you have experienced software collapse, and probably it was not a pleasant experience. In this paper, I will explain what software collapse is, what causes it, and how you can manage the risk of it happening to you. What I call software collapse is more commonly referred to as software rot: the fact that software stops working eventually if is not actively maintained. The rot metaphor has a long history, the first documented reference being the 1983 edition of the Hacker’s Dictionary. Back then, it was used jokingly by a small community of computer experts who understood the phenomenon perfectly well, and therefore a funny but technically inaccurate metaphor was not a problem. Today, it is being discussed in much wider circles, for example, in the context of reproducible research. In my opinion, it is appropriate to introduce a useful metaphor in place of the traditional humorous one, because good metaphors contribute to a better understanding of what is actually going on. The main issue with the rot metaphor is that it puts the blame on the wrong piece of the puzzle. If software becomes unusable over time, it is not because of any alteration to that software that needs to be reversed. Rather, it is the foundation on which the software has been built, ranging from the actual hardware via the operating system to programming languages and libraries, that has changed so much that the software is no longer compatible with it. Since unstable foundations resemble how a house is destroyed by an earthquake rather than how spoiling food is transformed by fungi, I consider collapse an appropriate metaphor.},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Hinsen, Konrad},
	month = may,
	year = {2019},
	note = {Conference Name: Computing in Science \& Engineering},
	keywords = {research software engineering, software collapse},
	pages = {104--108},
	file = {IEEE Xplore Abstract Record:/home/sam/Zotero/storage/6FX4X7BB/8701540.html:text/html;Submitted Version:/home/sam/Zotero/storage/J9AJJ73B/Hinsen - 2019 - Dealing With Software Collapse.pdf:application/pdf},
}

@article{davison_automated_2012,
	title = {Automated {Capture} of {Experiment} {Context} for {Easier} {Reproducibility} in {Computational} {Research}},
	volume = {14},
	issn = {1521-9615},
	url = {http://ieeexplore.ieee.org/document/6180156/},
	doi = {10.1109/MCSE.2012.41},
	abstract = {Published scientific research that relies on numerical computations is too often not reproducible. For computational research to become consistently and reliably reproducible, the process must become easier to achieve, as part of day-to-day research. A combination of best practices and automated tools can make it easier to create reproducible research.},
	number = {4},
	urldate = {2022-07-08},
	journal = {Computing in Science \& Engineering},
	author = {Davison, Andrew},
	month = jul,
	year = {2012},
	keywords = {reproducibility engineering, provenance},
	pages = {48--56},
	file = {Davison - 2012 - Automated Capture of Experiment Context for Easier.pdf:/home/sam/Zotero/storage/VBIVFYVD/Davison - 2012 - Automated Capture of Experiment Context for Easier.pdf:application/pdf},
}

@inproceedings{guo_cde_2011,
	address = {Portland, OR, USA},
	title = {{CDE}: {Using} {System} {Call} {Interposition} to {Automatically} {Create} {Portable} {Software} {Packages}},
	url = {https://www.usenix.org/legacy/events/atc11/tech/final_files/GuoEngler.pdf},
	abstract = {It can be painfully hard to take software that runs on one person’s machine and get it to run on another machine. Online forums and mailing lists are filled with discussions of users' troubles with compiling, installing, and configuring software and their myriad of dependencies. To eliminate this dependency problem, we created a system called CDE that uses system call interposition to monitor the execution of x86-Linux programs and package up the Code, Data, and Environment required to run them on other x86-Linux machines. Creating a CDE package is completely automatic, and running programs within a package requires no installation, configuration, or root permissions. Hundreds of people in both academia and industry have used CDE to distribute software, demo prototypes, make their scientific experiments reproducible, run software natively on older Linux distributions, and deploy experiments to compute clusters.},
	booktitle = {2011 {USENIX} {Annual} {Technical} {Conference}},
	publisher = {USENIX},
	author = {Guo, Philip and Engler, Dawson},
	month = jun,
	year = {2011},
	keywords = {reproducibility engineering},
}

@article{plesser_reproducibility_2018,
	title = {Reproducibility vs. {Replicability}: {A} {Brief} {History} of a {Confused} {Terminology}},
	volume = {11},
	issn = {1662-5196},
	shorttitle = {Reproducibility vs. {Replicability}},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2017.00076},
	urldate = {2022-10-11},
	journal = {Frontiers in Neuroinformatics},
	author = {Plesser, Hans E.},
	year = {2018},
	keywords = {reproducibility engineering},
	file = {Full Text PDF:/home/sam/Zotero/storage/JDIE62JR/Plesser - 2018 - Reproducibility vs. Replicability A Brief History.pdf:application/pdf},
}

@inproceedings{claerbout_electronic_1992,
	title = {Electronic documents give reproducible research a new meaning},
	url = {http://library.seg.org/doi/abs/10.1190/1.1822162},
	doi = {10.1190/1.1822162},
	language = {en},
	urldate = {2022-06-01},
	booktitle = {{SEG} {Technical} {Program} {Expanded} {Abstracts} 1992},
	publisher = {Society of Exploration Geophysicists},
	author = {Claerbout, Jon F. and Karrenbach, Martin},
	month = jan,
	year = {1992},
	keywords = {research software engineering, reproducibility engineering, internship-project},
	pages = {601--604},
}

@book{ritchie_science_2020,
	address = {New York},
	edition = {Illustrated edition},
	title = {Science {Fictions}: {How} {Fraud}, {Bias}, {Negligence}, and {Hype} {Undermine} the {Search} for {Truth}},
	isbn = {978-1-250-22269-5},
	shorttitle = {Science {Fictions}},
	language = {English},
	publisher = {Metropolitan Books},
	author = {Ritchie, Stuart},
	month = jul,
	year = {2020},
	keywords = {metascience},
}

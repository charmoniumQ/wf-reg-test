% cat ~/Downloads/Exported\ Items.bib | tee --append ~/box/wf-reg-test/docs/main.bib | head -n 2 | tail -n 1 | cut -f2 -d{ | cut -f1 -d, | xclip -selection clipboard


@misc{hettrick_softwaresavedsoftware_in_research_survey_2014_2018,
	title = {Softwaresaved/{Software}\_In\_Research\_Survey\_2014: {Software} {In} {Research} {Survey}},
	copyright = {Open Access},
	shorttitle = {Softwaresaved/{Software}\_In\_Research\_Survey\_2014},
	url = {https://zenodo.org/record/1183562},
	abstract = {This reproducible, Python-based re-analysis of the Software Sustainability Institute's 2014 research software survey. The original analysis was conducted in Excel, so this re-analysis was performed to improve the reproducibility of the results.},
	urldate = {2022-05-26},
	publisher = {Zenodo},
	author = {Hettrick, Simon},
	month = feb,
	year = {2018},
	doi = {10.5281/ZENODO.1183562},
	keywords = {research software engineering, internship-project},
}

@book{merton_sociology_1974,
	address = {Chicago},
	edition = {4. Dr.},
	title = {The sociology of science: theoretical and empirical investigations},
	isbn = {978-0-226-52092-6},
	shorttitle = {The sociology of science},
	language = {eng},
	publisher = {Univ. of Chicago Pr},
	author = {Merton, Robert K.},
	year = {1974},
	keywords = {internship-project, sociology},
}

@article{collberg_repeatability_2016,
	title = {Repeatability in computer systems research},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2812803},
	doi = {10.1145/2812803},
	abstract = {To encourage repeatable research, fund repeatability engineering and reward commitments to sharing research artifacts.},
	language = {en},
	number = {3},
	urldate = {2022-05-27},
	journal = {Communications of the ACM},
	author = {Collberg, Christian and Proebsting, Todd A.},
	month = feb,
	year = {2016},
	keywords = {research software engineering, internship-project},
	pages = {62--69},
	file = {2812803.pdf:/home/sam/Zotero/storage/JGDDR733/2812803.pdf:application/pdf},
}

@book{taylor_workflows_2014,
	edition = {2007th edition},
	title = {Workflows for e-{Science}: {Scientific} {Workflows} for {Grids}},
	isbn = {978-1-84996-619-1},
	shorttitle = {Workflows for e-{Science}},
	url = {https://link.springer.com/book/10.1007/978-1-84628-757-2},
	abstract = {Workflows for e-Science is divided into four parts, which represent four broad but distinct areas of scientific workflows. In the first part, Background, we introduce the concept of scientific workflows and set the scene by describing how they differ from their business workflow counterpart. In Part II, Application and User Perspective, we provide a number of scientific examples that currently use workflows for their e-Science experiments. In Workflow Representation and Common Structure (Part III), we describe core workflow themes, such as control flow or dataflow and the use of components or services. In this part, we also provide overviews for a number of common workflow languages, such as Petri Nets, the Business Process Execution Language (BPEL), and the Virtual Data Language (VDL), along with service interfaces. In Part IV, Frameworks and Tools, we take a look at many of the popular environments that are currently being used for e-Science applications by paying particular attention to their workflow capabilities. The following four sections describe the chapters in each part and therefore provide a comprehensive summary of the book as a whole.},
	language = {English},
	publisher = {Springer},
	editor = {Taylor, Ian J. and Deelman, Ewa and Gannon, Dennis B. and Shields, Matthew},
	month = mar,
	year = {2014},
	keywords = {workflow managers},
	file = {Snapshot:/home/sam/Zotero/storage/92UFJE7C/978-1-84628-757-2.html:text/html;Taylor et al. - Workflows for e-Science.pdf:/home/sam/Zotero/storage/9KLYDT65/Taylor et al. - Workflows for e-Science.pdf:application/pdf},
}

@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2022-09-14},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	note = {Publisher: Public Library of Science},
	pages = {e124},
	file = {Full Text PDF:/home/sam/Zotero/storage/YNN44PSX/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf;Snapshot:/home/sam/Zotero/storage/BKJXJ6QD/article.html:text/html},
}

@article{gil_examining_2007,
	title = {Examining the {Challenges} of {Scientific} {Workflows}},
	volume = {40},
	issn = {1558-0814},
	doi = {10.1109/MC.2007.421},
	abstract = {Workflows have emerged as a paradigm for representing and managing complex distributed computations and are used to accelerate the pace of scientific progress. A recent National Science Foundation workshop brought together domain, computer, and social scientists to discuss requirements of future scientific applications and the challenges they present to current workflow technologies.},
	number = {12},
	journal = {Computer},
	author = {Gil, Yolanda and Deelman, Ewa and Ellisman, Mark and Fahringer, Thomas and Fox, Geoffrey and Gannon, Dennis and Goble, Carole and Livny, Miron and Moreau, Luc and Myers, Jim},
	month = dec,
	year = {2007},
	note = {Conference Name: Computer
interest: 91},
	keywords = {workflow managers},
	pages = {24--32},
	file = {Accepted Version:/home/sam/Zotero/storage/SHKAM8S5/Gil et al. - 2007 - Examining the Challenges of Scientific Workflows.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sam/Zotero/storage/FPVVU7W6/4404805.html:text/html},
}

@inproceedings{henkel_shipwright_2021,
	title = {Shipwright: {A} {Human}-in-the-{Loop} {System} for {Dockerfile} {Repair}},
	shorttitle = {Shipwright},
	doi = {10.1109/ICSE43902.2021.00106},
	abstract = {Docker is a tool for lightweight OS-level virtualization. Docker images are created by performing a build, controlled by a source-level artifact called a Dockerfile. We studied Dockerfiles on GitHub, and-to our great surprise-found that over a quarter of the examined Dockerfiles failed to build (and thus to produce images). To address this problem, we propose SHIPWRIGHT, a human-in-the-loop system for finding repairs to broken Dockerfiles. SHIPWRIGHT uses a modified version of the BERT language model to embed build logs and to cluster broken Dockerfiles. Using these clusters and a search-based procedure, we were able to design 13 rules for making automated repairs to Dockerfiles. With the aid of SHIPWRIGHT, we submitted 45 pull requests (with a 42.2\% acceptance rate) to GitHub projects with broken Dockerfiles. Furthermore, in a "time-travel" analysis of broken Dockerfiles that were later fixed, we found that SHIPWRIGHT proposed repairs that were equivalent to human-authored patches in 22.77\% of the cases we studied. Finally, we compared our work with recent, state-of-the-art, static Dockerfile analyses, and found that, while static tools detected possible build-failure-inducing issues in 20.6-33.8\% of the files we examined, SHIPWRIGHT was able to detect possible issues in 73.25\% of the files and, additionally, provide automated repairs for 18.9\% of the files.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Henkel, Jordan and Silva, Denini and Teixeira, Leopoldo and d’Amorim, Marcelo and Reps, Thomas},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {reproducibility engineering},
	pages = {1148--1160},
	file = {IEEE Xplore Full Text PDF:/home/sam/Zotero/storage/ME8QXGUQ/Henkel et al. - 2021 - Shipwright A Human-in-the-Loop System for Dockerf.pdf:application/pdf},
}

@inproceedings{zhao_why_2012,
	title = {Why workflows break — understanding and combating decay in {Taverna} workflows},
	url = {https://www.research.manchester.ac.uk/portal/en/publications/why-workflows-break--understanding-and-combating-decay-in-taverna-workflows(cba81ca4-e92c-408e-8442-383d1f15fcdf)/export.html},
	doi = {10.1109/eScience.2012.6404482},
	abstract = {Workflows provide a popular means for preserving scientific methods by explicitly encoding their process. However, some of them are subject to a decay in their ability to be re-executed or reproduce the same results over time, largely due to the volatility of the resources required for workflow executions. This paper provides an analysis of the root causes of workflow decay based on an empirical study of a collection of Taverna workflows from the myExperiment repository. Although our analysis was based on a specific type of workflow, the outcomes and methodology should be applicable to workflows from other systems, at least those whose executions also rely largely on accessing third-party resources. Based on our understanding about decay we recommend a minimal set of auxiliary resources to be preserved together with the workflows as an aggregation object and provide a software tool for end-users to create such aggregations and to assess their completeness},
	booktitle = {2012 {IEEE} 8th {International} {Conference} on {E}-{Science} (e-{Science})},
	author = {Zhao, Jun and Gomez-Perez, Jose-Manuel and Belhajjame, Khalid and Klyne, Graham and Garcia-cuesta, Esteban and Garrido, Aleix and Hettne, Kristina and Roos, Marco and De Roure, David and Goble, Carole},
	month = oct,
	year = {2012},
	note = {interest: 99},
	keywords = {workflow managers, internship-project},
	pages = {9},
	file = {Why_workflows_break__Understanding_and_combating_decay_in_Taverna_workflows.pdf:/home/sam/Zotero/storage/2BQSSKJF/Why_workflows_break__Understanding_and_combating_decay_in_Taverna_workflows.pdf:application/pdf},
}

@article{hinsen_dealing_2019,
	title = {Dealing {With} {Software} {Collapse}},
	volume = {21},
	issn = {1558-366X},
	doi = {10.1109/MCSE.2019.2900945},
	abstract = {Discusses the concept of software collapse. There is A good chance that you have never heard of software collapse before, for the simple reason that it is a term I have made up myself two years ago in a blog post. However, if you have been doing computational science for a few years, there is a good chance that you have experienced software collapse, and probably it was not a pleasant experience. In this paper, I will explain what software collapse is, what causes it, and how you can manage the risk of it happening to you. What I call software collapse is more commonly referred to as software rot: the fact that software stops working eventually if is not actively maintained. The rot metaphor has a long history, the first documented reference being the 1983 edition of the Hacker’s Dictionary. Back then, it was used jokingly by a small community of computer experts who understood the phenomenon perfectly well, and therefore a funny but technically inaccurate metaphor was not a problem. Today, it is being discussed in much wider circles, for example, in the context of reproducible research. In my opinion, it is appropriate to introduce a useful metaphor in place of the traditional humorous one, because good metaphors contribute to a better understanding of what is actually going on. The main issue with the rot metaphor is that it puts the blame on the wrong piece of the puzzle. If software becomes unusable over time, it is not because of any alteration to that software that needs to be reversed. Rather, it is the foundation on which the software has been built, ranging from the actual hardware via the operating system to programming languages and libraries, that has changed so much that the software is no longer compatible with it. Since unstable foundations resemble how a house is destroyed by an earthquake rather than how spoiling food is transformed by fungi, I consider collapse an appropriate metaphor.},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Hinsen, Konrad},
	month = may,
	year = {2019},
	note = {Conference Name: Computing in Science \& Engineering},
	keywords = {research software engineering, software collapse},
	pages = {104--108},
	file = {IEEE Xplore Abstract Record:/home/sam/Zotero/storage/6FX4X7BB/8701540.html:text/html;Submitted Version:/home/sam/Zotero/storage/J9AJJ73B/Hinsen - 2019 - Dealing With Software Collapse.pdf:application/pdf},
}

@article{davison_automated_2012,
	title = {Automated {Capture} of {Experiment} {Context} for {Easier} {Reproducibility} in {Computational} {Research}},
	volume = {14},
	issn = {1521-9615},
	url = {http://ieeexplore.ieee.org/document/6180156/},
	doi = {10.1109/MCSE.2012.41},
	abstract = {Published scientific research that relies on numerical computations is too often not reproducible. For computational research to become consistently and reliably reproducible, the process must become easier to achieve, as part of day-to-day research. A combination of best practices and automated tools can make it easier to create reproducible research.},
	number = {4},
	urldate = {2022-07-08},
	journal = {Computing in Science \& Engineering},
	author = {Davison, Andrew},
	month = jul,
	year = {2012},
	keywords = {reproducibility engineering, provenance},
	pages = {48--56},
	file = {Davison - 2012 - Automated Capture of Experiment Context for Easier.pdf:/home/sam/Zotero/storage/VBIVFYVD/Davison - 2012 - Automated Capture of Experiment Context for Easier.pdf:application/pdf},
}

@inproceedings{guo_cde_2011,
	address = {Portland, OR, USA},
	title = {{CDE}: {Using} {System} {Call} {Interposition} to {Automatically} {Create} {Portable} {Software} {Packages}},
	url = {https://www.usenix.org/legacy/events/atc11/tech/final_files/GuoEngler.pdf},
	abstract = {It can be painfully hard to take software that runs on one person’s machine and get it to run on another machine. Online forums and mailing lists are filled with discussions of users' troubles with compiling, installing, and configuring software and their myriad of dependencies. To eliminate this dependency problem, we created a system called CDE that uses system call interposition to monitor the execution of x86-Linux programs and package up the Code, Data, and Environment required to run them on other x86-Linux machines. Creating a CDE package is completely automatic, and running programs within a package requires no installation, configuration, or root permissions. Hundreds of people in both academia and industry have used CDE to distribute software, demo prototypes, make their scientific experiments reproducible, run software natively on older Linux distributions, and deploy experiments to compute clusters.},
	booktitle = {2011 {USENIX} {Annual} {Technical} {Conference}},
	publisher = {USENIX},
	author = {Guo, Philip and Engler, Dawson},
	month = jun,
	year = {2011},
	keywords = {reproducibility engineering},
}

@article{plesser_reproducibility_2018,
	title = {Reproducibility vs. {Replicability}: {A} {Brief} {History} of a {Confused} {Terminology}},
	volume = {11},
	issn = {1662-5196},
	shorttitle = {Reproducibility vs. {Replicability}},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2017.00076},
	urldate = {2022-10-11},
	journal = {Frontiers in Neuroinformatics},
	author = {Plesser, Hans E.},
	year = {2018},
	keywords = {reproducibility engineering},
	file = {Full Text PDF:/home/sam/Zotero/storage/JDIE62JR/Plesser - 2018 - Reproducibility vs. Replicability A Brief History.pdf:application/pdf},
}

@inproceedings{claerbout_electronic_1992,
	title = {Electronic documents give reproducible research a new meaning},
	url = {http://library.seg.org/doi/abs/10.1190/1.1822162},
	doi = {10.1190/1.1822162},
	language = {en},
	urldate = {2022-06-01},
	booktitle = {{SEG} {Technical} {Program} {Expanded} {Abstracts} 1992},
	publisher = {Society of Exploration Geophysicists},
	author = {Claerbout, Jon F. and Karrenbach, Martin},
	month = jan,
	year = {1992},
	keywords = {research software engineering, reproducibility engineering, internship-project},
	pages = {601--604},
}

@book{ritchie_science_2020,
	address = {New York},
	edition = {Illustrated edition},
	title = {Science {Fictions}: {How} {Fraud}, {Bias}, {Negligence}, and {Hype} {Undermine} the {Search} for {Truth}},
	isbn = {978-1-250-22269-5},
	shorttitle = {Science {Fictions}},
	language = {English},
	publisher = {Metropolitan Books},
	author = {Ritchie, Stuart},
	month = jul,
	year = {2020},
	keywords = {metascience},
}

@article{collberg_repeatability_2016,
	title = {Repeatability in computer systems research},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2812803},
	doi = {10.1145/2812803},
	abstract = {To encourage repeatable research, fund repeatability engineering and reward commitments to sharing research artifacts.},
	language = {en},
	number = {3},
	urldate = {2022-05-27},
	journal = {Communications of the ACM},
	author = {Collberg, Christian and Proebsting, Todd A.},
	month = feb,
	year = {2016},
	keywords = {research software engineering, internship-project},
	pages = {62--69},
	file = {2812803.pdf:/home/sam/Zotero/storage/JGDDR733/2812803.pdf:application/pdf},
}

@inproceedings{dutta_flex_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {{FLEX}: fixing flaky tests in machine learning projects by updating assertion bounds},
	isbn = {978-1-4503-8562-6},
	shorttitle = {{FLEX}},
	url = {https://doi.org/10.1145/3468264.3468615},
	doi = {10.1145/3468264.3468615},
	abstract = {Many machine learning (ML) algorithms are inherently random – multiple executions using the same inputs may produce slightly different results each time. Randomness impacts how developers write tests that check for end-to-end quality of their implementations of these ML algorithms. In particular, selecting the proper thresholds for comparing obtained quality metrics with the reference results is a non-intuitive task, which may lead to flaky test executions. We present FLEX, the first tool for automatically fixing flaky tests due to algorithmic randomness in ML algorithms. FLEX fixes tests that use approximate assertions to compare actual and expected values that represent the quality of the outputs of ML algorithms. We present a technique for systematically identifying the acceptable bound between the actual and expected output quality that also minimizes flakiness. Our technique is based on the Peak Over Threshold method from statistical Extreme Value Theory, which estimates the tail distribution of the output values observed from several runs. Based on the tail distribution, FLEX updates the bound used in the test, or selects the number of test re-runs, based on a desired confidence level. We evaluate FLEX on a corpus of 35 tests collected from the latest versions of 21 ML projects. Overall, FLEX identifies and proposes a fix for 28 tests. We sent 19 pull requests, each fixing one test, to the developers. So far, 9 have been accepted by the developers.},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Dutta, Saikat and Shi, August and Misailovic, Sasa},
	month = aug,
	year = {2021},
	note = {interest: 97},
	keywords = {reproducibility engineering, software testing},
	pages = {603--614},
	file = {Full Text PDF:/home/sam/Zotero/storage/MVEFQYJY/Dutta et al. - 2021 - FLEX fixing flaky tests in machine learning proje.pdf:application/pdf},
}

@unpublished{vu_outcome-preserving_2022,
	address = {Rochester, MI},
	title = {Outcome-{Preserving} {Input} {Reduction} for {Scientific} {Data} {Analysis} {Workflows}},
	url = {https://seg.inf.unibe.ch/papers/ase22.pdf},
	abstract = {Analysis of data is the foundation of multiple scientific disciplines,
manifesting in complex and diverse scientific data analysis work-
flows often involving exploratory analyses. Such analyses represent
a particular case for traditional data engineering workflows, as re-
sults may be hard to interpret and judge whether they are correct
or not, and where experimentation is a central theme. Oftentimes,
there are certain aspects of a result which are suspicious and which
should be further investigated to increase the trustworthiness of the
workflow’s outcome. To this end, we advocate a semi-automated
approach to reducing a workflow’s input data while preserving a
specified outcome of interest, facilitating irregularity localization
by narrowing down the search space for spotting corrupted input
data or wrong assumptions made about it. We outline our vision
on building engineering support for outcome-preserving input re-
duction within data analysis workflows, and report on preliminary
results obtained from applying an early research prototype on a
computational notebook taken from an online community of data
scientists and machine learning practitioners.},
	author = {Vu, Anh Duc and Kehrer, Timo and Tsigkanos, Christos},
	month = oct,
	year = {2022},
	note = {interest: 99},
}

@article{ewels_nf-core_2020,
	title = {The nf-core framework for community-curated bioinformatics pipelines},
	volume = {38},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-020-0439-x},
	doi = {10.1038/s41587-020-0439-x},
	language = {en},
	number = {3},
	urldate = {2022-10-31},
	journal = {Nature Biotechnology},
	author = {Ewels, Philip A. and Peltzer, Alexander and Fillinger, Sven and Patel, Harshil and Alneberg, Johannes and Wilm, Andreas and Garcia, Maxime Ulysse and Di Tommaso, Paolo and Nahnsen, Sven},
	month = mar,
	year = {2020},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {reproducibility engineering, workflows},
	pages = {276--278},
	file = {Full Text PDF:/home/sam/Zotero/storage/H4EZXAF5/Ewels et al. - 2020 - The nf-core framework for community-curated bioinf.pdf:application/pdf;Snapshot:/home/sam/Zotero/storage/LQDQZTUV/s41587-020-0439-x.html:text/html},
}

@article{oconnor_dockstore_2017,
	title = {The {Dockstore}: enabling modular, community-focused sharing of {Docker}-based genomics tools and workflows},
	volume = {6},
	issn = {2046-1402},
	shorttitle = {The {Dockstore}},
	url = {https://f1000research.com/articles/6-52/v1},
	doi = {10.12688/f1000research.10137.1},
	abstract = {As genomic datasets continue to grow, the feasibility of downloading data to a local organization and running analysis on a traditional compute environment is becoming increasingly problematic. Current large-scale projects, such as the ICGC PanCancer Analysis of Whole Genomes (PCAWG), the Data Platform for the U.S. Precision Medicine Initiative, and the NIH Big Data to Knowledge Center for Translational Genomics, are using cloud-based infrastructure to both host and perform analysis across large data sets. In PCAWG, over 5,800 whole human genomes were aligned and variant called across 14 cloud and HPC environments; the processed data was then made available on the cloud for further analysis and sharing. If run locally, an operation at this scale would have monopolized a typical academic data centre for many months, and would have presented major challenges for data storage and distribution. However, this scale is increasingly typical for genomics projects and necessitates a rethink of how analytical tools are packaged and moved to the data. For PCAWG, we embraced the use of highly portable Docker images for encapsulating and sharing complex alignment and variant calling workflows across highly variable environments. While successful, this endeavor revealed a limitation in Docker containers, namely the lack of a standardized way to describe and execute the tools encapsulated inside the container. As a result, we created the Dockstore (
              https://dockstore.org
              ), a project that brings together Docker images with standardized, machine-readable ways of describing and running the tools contained within. This service greatly improves the sharing and reuse of genomics tools and promotes interoperability with similar projects through emerging web service standards developed by the Global Alliance for Genomics and Health (GA4GH).},
	language = {en},
	urldate = {2022-10-31},
	journal = {F1000Research},
	author = {O'Connor, Brian D. and Yuen, Denis and Chung, Vincent and Duncan, Andrew G. and Liu, Xiang Kun and Patricia, Janice and Paten, Benedict and Stein, Lincoln and Ferretti, Vincent},
	month = jan,
	year = {2017},
	keywords = {workflows},
	pages = {52},
	file = {Full Text:/home/sam/Zotero/storage/A2EM7HZ5/O'Connor et al. - 2017 - The Dockstore enabling modular, community-focused.pdf:application/pdf},
}

@inproceedings{silva_workflowhub_2020,
	title = {{WorkflowHub}: {Community} {Framework} for {Enabling} {Scientific} {Workflow} {Research} and {Development}},
	shorttitle = {{WorkflowHub}},
	doi = {10.1109/WORKS51914.2020.00012},
	abstract = {Scientific workflows are a cornerstone of modern scientific computing. They are used to describe complex computational applications that require efficient and robust management of large volumes of data, which are typically stored/processed on heterogeneous, distributed resources. The workflow research and development community has employed a number of methods for the quantitative evaluation of existing and novel workflow algorithms and systems. In particular, a common approach is to simulate workflow executions. In previous work, we have presented a collection of tools that have been used for aiding research and development activities in the Pegasus project, and that have been adopted by others for conducting workflow research. Despite their popularity, there are several shortcomings that prevent easy adoption, maintenance, and consistency with the evolving structures and computational requirements of production workflows. In this work, we present WorkflowHub, a community framework that provides a collection of tools for analyzing workflow execution traces, producing realistic synthetic workflow traces, and simulating workflow executions. We demonstrate the realism of the generated synthetic traces by comparing simulated executions of these traces with actual workflow executions. We also contrast these results with those obtained when using the previously available collection of tools. We find that our framework not only can be used to generate representative synthetic workflow traces (i.e., with workflow structures and task characteristics distributions that resemble those in traces obtained from real-world workflow executions), but can also generate representative workflow traces at larger scales than that of available workflow traces.},
	booktitle = {2020 {IEEE}/{ACM} {Workflows} in {Support} of {Large}-{Scale} {Science} ({WORKS})},
	author = {Silva, Rafael Ferreira da and Pottier, Loïc and Coleman, Tainã and Deelman, Ewa and Casanova, Henri},
	month = nov,
	year = {2020},
	keywords = {workflows},
	pages = {49--56},
	file = {IEEE Xplore Full Text PDF:/home/sam/Zotero/storage/262NG8DJ/Silva et al. - 2020 - WorkflowHub Community Framework for Enabling Scie.pdf:application/pdf},
}

@article{goble_myexperiment_2010,
	title = {{myExperiment}: a repository and social network for the sharing of bioinformatics workflows},
	volume = {38},
	issn = {0305-1048},
	shorttitle = {{myExperiment}},
	url = {https://doi.org/10.1093/nar/gkq429},
	doi = {10.1093/nar/gkq429},
	abstract = {myExperiment (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, myExperiment allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to myExperiment and enable them to be shared in a secure manner. Since its release in 2007, myExperiment currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project. Contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work. Further documentation about myExperiment including its REST web service is available from http://wiki.myexperiment.org. Feedback and requests for support can be sent to bugs@myexperiment.org.},
	number = {suppl\_2},
	urldate = {2022-10-31},
	journal = {Nucleic Acids Research},
	author = {Goble, Carole A. and Bhagat, Jiten and Aleksejevs, Sergejs and Cruickshank, Don and Michaelides, Danius and Newman, David and Borkum, Mark and Bechhofer, Sean and Roos, Marco and Li, Peter and De Roure, David},
	month = jul,
	year = {2010},
	keywords = {workflows},
	pages = {W677--W682},
	file = {Full Text PDF:/home/sam/Zotero/storage/JWIDGCGY/Goble et al. - 2010 - myExperiment a repository and social network for .pdf:application/pdf},
}

@article{coleman_wfcommons_2022,
	title = {{WfCommons}: {A} framework for enabling scientific workflow research and development},
	volume = {128},
	issn = {0167-739X},
	shorttitle = {{WfCommons}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X21003897},
	doi = {10.1016/j.future.2021.09.043},
	abstract = {Scientific workflows are a cornerstone of modern scientific computing. They are used to describe complex computational applications that require efficient and robust management of large volumes of data, which are typically stored/processed on heterogeneous, distributed resources. The workflow research and development community has employed a number of methods for the quantitative evaluation of existing and novel workflow algorithms and systems. In particular, a common approach is to simulate workflow executions. In previous works, we have presented a collection of tools that have been adopted by the community for conducting workflow research. Despite their popularity, they suffer from several shortcomings that prevent easy adoption, maintenance, and consistency with the evolving structures and computational requirements of production workflows. In this work, we present WfCommons , a framework that provides a collection of tools for analyzing workflow executions, for producing generators of synthetic workflows, and for simulating workflow executions. We demonstrate the realism of the generated synthetic workflows by comparing their simulated executions to real workflow executions. We also contrast these results with results obtained when using the previously available collection of tools. We find that the workflow generators that are automatically constructed by our framework not only generate representative same-scale workflows (i.e., with structures and task characteristics distributions that resemble those observed in real-world workflows), but also do so at scales larger than that of available real-world workflows. Finally, we conduct a case study to demonstrate the usefulness of our framework for estimating the energy consumption of large-scale workflow executions.},
	language = {en},
	urldate = {2022-10-31},
	journal = {Future Generation Computer Systems},
	author = {Coleman, Tainã and Casanova, Henri and Pottier, Loïc and Kaushik, Manav and Deelman, Ewa and Ferreira da Silva, Rafael},
	month = mar,
	year = {2022},
	keywords = {workflows},
	pages = {16--27},
	file = {ScienceDirect Full Text PDF:/home/sam/Zotero/storage/Q8GA8Z3U/Coleman et al. - 2022 - WfCommons A framework for enabling scientific wor.pdf:application/pdf;ScienceDirect Snapshot:/home/sam/Zotero/storage/6GBIB86Y/S0167739X21003897.html:text/html},
}

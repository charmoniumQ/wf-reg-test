
@inproceedings{gamblin_spack_2015,
	address = {New York, NY, USA},
	series = {{SC} '15},
	title = {The {Spack} package manager: bringing order to {HPC} software chaos},
	isbn = {978-1-4503-3723-6},
	shorttitle = {The {Spack} package manager},
	url = {https://doi.org/10.1145/2807591.2807623},
	doi = {10.1145/2807591.2807623},
	abstract = {Large HPC centers spend considerable time supporting software for thousands of users, but the complexity of HPC software is quickly outpacing the capabilities of existing software management tools. Scientific applications require specific versions of compilers, MPI, and other dependency libraries, so using a single, standard software stack is infeasible. However, managing many configurations is difficult because the configuration space is combinatorial in size. We introduce Spack, a tool used at Lawrence Livermore National Laboratory to manage this complexity. Spack provides a novel, recursive specification syntax to invoke parametric builds of packages and dependencies. It allows any number of builds to coexist on the same system, and it ensures that installed packages can find their dependencies, regardless of the environment. We show through real-world use cases that Spack supports diverse and demanding applications, bringing order to HPC software chaos.},
	urldate = {2022-04-10},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Gamblin, Todd and LeGendre, Matthew and Collette, Michael R. and Lee, Gregory L. and Moody, Adam and de Supinski, Bronis R. and Futral, Scott},
	month = nov,
	year = {2015},
	note = {interest: 80},
	keywords = {research software engineering, high-performance computing, package managers, operating systems, reproducibility engineering, project-acm-rep, project-astrophysics},
	pages = {1--12},
	file = {Full Text PDF:/home/sam/Zotero/storage/7RMEVM2B/Gamblin et al. - 2015 - The Spack package manager bringing order to HPC s.pdf:application/pdf},
}

@article{mytkowicz_producing_2009,
	title = {Producing wrong data without doing anything obviously wrong!},
	volume = {37},
	issn = {0163-5964},
	url = {https://doi.org/10.1145/2528521.1508275},
	doi = {10.1145/2528521.1508275},
	abstract = {This paper presents a surprising result: changing a seemingly innocuous aspect of an experimental setup can cause a systems researcher to draw wrong conclusions from an experiment. What appears to be an innocuous aspect in the experimental setup may in fact introduce a significant bias in an evaluation. This phenomenon is called measurement bias in the natural and social sciences. Our results demonstrate that measurement bias is significant and commonplace in computer system evaluation. By significant we mean that measurement bias can lead to a performance analysis that either over-states an effect or even yields an incorrect conclusion. By commonplace we mean that measurement bias occurs in all architectures that we tried (Pentium 4, Core 2, and m5 O3CPU), both compilers that we tried (gcc and Intel's C compiler), and most of the SPEC CPU2006 C programs. Thus, we cannot ignore measurement bias. Nevertheless, in a literature survey of 133 recent papers from ASPLOS, PACT, PLDI, and CGO, we determined that none of the papers with experimental results adequately consider measurement bias. Inspired by similar problems and their solutions in other sciences, we describe and demonstrate two methods, one for detecting (causal analysis) and one for avoiding (setup randomization) measurement bias.},
	number = {1},
	urldate = {2022-04-11},
	journal = {SIGARCH Comput. Archit. News},
	author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
	month = mar,
	year = {2009},
	note = {score: 95
interest: 70},
	keywords = {software benchmarking, software engineering, project-acm-rep},
	pages = {265--276},
	file = {1508284.1508275.pdf:/home/sam/Zotero/storage/WRXTAESN/1508284.1508275.pdf:application/pdf},
}

@inproceedings{babuji_parsl_2019,
	address = {New York, NY, USA},
	series = {{HPDC} '19},
	title = {Parsl: {Pervasive} {Parallel} {Programming} in {Python}},
	isbn = {978-1-4503-6670-0},
	shorttitle = {Parsl},
	url = {https://doi.org/10.1145/3307681.3325400},
	doi = {10.1145/3307681.3325400},
	abstract = {High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.},
	urldate = {2022-05-12},
	booktitle = {Proceedings of the 28th {International} {Symposium} on {High}-{Performance} {Parallel} and {Distributed} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle},
	month = jun,
	year = {2019},
	keywords = {workflow managers, project-acm-rep, project-charmonium.cache},
	pages = {25--36},
	file = {Full Text PDF:/home/sam/Zotero/storage/8RGEIJIE/Babuji et al. - 2019 - Parsl Pervasive Parallel Programming in Python.pdf:application/pdf},
}

@article{collberg_repeatability_2016,
	title = {Repeatability in computer systems research},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2812803},
	doi = {10.1145/2812803},
	abstract = {To encourage repeatable research, fund repeatability engineering and reward commitments to sharing research artifacts.},
	language = {en},
	number = {3},
	urldate = {2022-05-27},
	journal = {Commun. ACM},
	author = {Collberg, Christian and Proebsting, Todd A.},
	month = feb,
	year = {2016},
	keywords = {research software engineering, internship-project, project-acm-rep},
	pages = {62--69},
	file = {2812803.pdf:/home/sam/Zotero/storage/JGDDR733/2812803.pdf:application/pdf},
}

@book{merton_sociology_1974,
	address = {Chicago},
	edition = {4. Dr.},
	title = {The sociology of science: theoretical and empirical investigations},
	isbn = {978-0-226-52092-6},
	shorttitle = {The sociology of science},
	language = {eng},
	publisher = {Univ. of Chicago Pr},
	author = {Merton, Robert K.},
	year = {1974},
	keywords = {internship-project, sociology, project-acm-rep},
}

@article{herndon_does_2014,
	title = {Does high public debt consistently stifle economic growth? {A} critique of {Reinhart} and {Rogoff}},
	volume = {38},
	issn = {0309-166X, 1464-3545},
	shorttitle = {Does high public debt consistently stifle economic growth?},
	url = {https://academic.oup.com/cje/article-lookup/doi/10.1093/cje/bet075},
	doi = {10.1093/cje/bet075},
	abstract = {We replicate Reinhart and Rogoff (2010A and 2010B) and find that selective exclusion of available data, coding errors and inappropriate weighting of summary statistics lead to serious miscalculations that inaccurately represent the relationship between public debt and GDP growth among 20 advanced economies. Over 1946–2009, countries with public debt/GDP ratios above 90\% averaged 2.2\% real annual GDP growth, not −0.1\% as published. The published results for (i) median GDP growth rates for the 1946–2009 period and (ii) mean and median GDP growth figures over 1790–2009 are all distorted by similar methodological errors, although the magnitudes of the distortions are somewhat smaller than with the mean figures for 1946–2009. Contrary to Reinhart and Rogoff’s broader contentions, both mean and median GDP growth when public debt levels exceed 90\% of GDP are not dramatically different from when the public debt/GDP ratios are lower. The relationship between public debt and GDP growth varies significantly by period and country. Our overall evidence refutes RR’s claim that public debt/GDP ratios above 90\% consistently reduce a country’s GDP growth.},
	language = {en},
	number = {2},
	urldate = {2022-05-26},
	journal = {Cambridge Journal of Economics},
	author = {Herndon, T. and Ash, M. and Pollin, R.},
	month = mar,
	year = {2014},
	keywords = {research software engineering, internship-project, project-acm-rep},
	pages = {257--279},
	file = {bet075.pdf:/home/sam/Zotero/storage/4I3PZ5XK/bet075.pdf:application/pdf},
}

@article{neupane_characterization_2019,
	title = {Characterization of {Leptazolines} {A}–{D}, {Polar} {Oxazolines} from the {Cyanobacterium} {Leptolyngbya} sp., {Reveals} a {Glitch} with the “{Willoughby}–{Hoye}” {Scripts} for {Calculating} {NMR} {Chemical} {Shifts}},
	volume = {21},
	issn = {1523-7060, 1523-7052},
	url = {https://pubs.acs.org/doi/10.1021/acs.orglett.9b03216},
	doi = {10.1021/acs.orglett.9b03216},
	abstract = {The bioactivity-guided examination of a Leptolyngbya sp. led to the isolation of leptazolines A–D (1–4), from the culture media, along with two degradation products (5 and 6). Density functional theory nuclear magnetic resonance calculations established the relative configurations of 1 and 2 and revealed that the calculated shifts depended on the operating system when using the “Willoughby–Hoye” Python scripts to streamline the processing of the output files, a previously unrecognized flaw that could lead to incorrect conclusions.},
	language = {en},
	number = {20},
	urldate = {2022-05-26},
	journal = {Org. Lett.},
	author = {Neupane, Jayanti Bhandari and Neupane, Ram P. and Luo, Yuheng and Yoshida, Wesley Y. and Sun, Rui and Williams, Philip G.},
	month = oct,
	year = {2019},
	keywords = {research software engineering, internship-project, project-acm-rep, retraction},
	pages = {8449--8453},
	file = {acs.orglett.9b03216.pdf:/home/sam/Zotero/storage/QWATD4UA/acs.orglett.9b03216.pdf:application/pdf},
}

@article{miller_scientists_2006,
	title = {A {Scientist}'s {Nightmare}: {Software} {Problem} {Leads} to {Five} {Retractions}},
	volume = {314},
	issn = {0036-8075, 1095-9203},
	shorttitle = {A {Scientist}'s {Nightmare}},
	url = {https://www.science.org/doi/10.1126/science.314.5807.1856},
	doi = {10.1126/science.314.5807.1856},
	language = {en},
	number = {5807},
	urldate = {2022-05-26},
	journal = {Science},
	author = {Miller, Greg},
	month = dec,
	year = {2006},
	keywords = {research software engineering, project-acm-rep, retraction},
	pages = {1856--1857},
	file = {science.314.5807.1856.pdf:/home/sam/Zotero/storage/6VPQSDMX/science.314.5807.1856.pdf:application/pdf},
}

@inproceedings{priedhorsky_charliecloud_2017,
	address = {Denver Colorado},
	title = {Charliecloud: unprivileged containers for user-defined software stacks in {HPC}},
	isbn = {978-1-4503-5114-0},
	shorttitle = {Charliecloud},
	url = {https://dl.acm.org/doi/10.1145/3126908.3126925},
	doi = {10.1145/3126908.3126925},
	abstract = {Supercomputing centers are seeing increasing demand for user-defined software stacks (UDSS), instead of or in addition to the stack provided by the center. These UDSS support user needs such as complex dependencies or build requirements, externally required configurations, portability, and consistency. The challenge for centers is to provide these services in a usable manner while minimizing the risks: security, support burden, missing functionality, and performance. We present Charliecloud, which uses the Linux user and mount namespaces to run industry-standard Docker containers with no privileged operations or daemons on center resources. Our simple approach avoids most security risks while maintaining access to the performance and functionality already on offer, doing so in just 800 lines of code. Charliecloud promises to bring an industry-standard UDSS user workflow to existing, minimally altered HPC resources.},
	language = {en},
	urldate = {2022-05-26},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {ACM},
	author = {Priedhorsky, Reid and Randles, Tim},
	month = nov,
	year = {2017},
	keywords = {containers, hpc, operating systems, project-acm-rep},
	pages = {1--10},
	file = {3126908.3126925.pdf:/home/sam/Zotero/storage/T2VWSVNT/3126908.3126925.pdf:application/pdf},
}

@inproceedings{zhao_why_2012,
	address = {Chicago, IL},
	title = {Why workflows break — understanding and combating decay in {Taverna} workflows},
	url = {https://www.research.manchester.ac.uk/portal/en/publications/why-workflows-break--understanding-and-combating-decay-in-taverna-workflows(cba81ca4-e92c-408e-8442-383d1f15fcdf)/export.html},
	doi = {10.1109/eScience.2012.6404482},
	abstract = {Workflows provide a popular means for preserving scientific methods by explicitly encoding their process. However, some of them are subject to a decay in their ability to be re-executed or reproduce the same results over time, largely due to the volatility of the resources required for workflow executions. This paper provides an analysis of the root causes of workflow decay based on an empirical study of a collection of Taverna workflows from the myExperiment repository. Although our analysis was based on a specific type of workflow, the outcomes and methodology should be applicable to workflows from other systems, at least those whose executions also rely largely on accessing third-party resources. Based on our understanding about decay we recommend a minimal set of auxiliary resources to be preserved together with the workflows as an aggregation object and provide a software tool for end-users to create such aggregations and to assess their completeness},
	booktitle = {2012 {IEEE} 8th {International} {Conference} on {E}-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Zhao, Jun and Gomez-Perez, Jose-Manuel and Belhajjame, Khalid and Klyne, Graham and Garcia-cuesta, Esteban and Garrido, Aleix and Hettne, Kristina and Roos, Marco and De Roure, David and Goble, Carole},
	month = oct,
	year = {2012},
	keywords = {workflow managers, internship-project, project-acm-rep},
	pages = {9},
	file = {Why_workflows_break__Understanding_and_combating_decay_in_Taverna_workflows.pdf:/home/sam/Zotero/storage/2BQSSKJF/Why_workflows_break__Understanding_and_combating_decay_in_Taverna_workflows.pdf:application/pdf},
}

@inproceedings{ferreira_da_silva_community_2021,
	address = {St. Louis, MO, USA},
	title = {A {Community} {Roadmap} for {Scientific} {Workflows} {Research} and {Development}},
	isbn = {978-1-66541-136-3},
	url = {https://ieeexplore.ieee.org/document/9652570/},
	doi = {10.1109/WORKS54523.2021.00016},
	abstract = {The landscape of workflow systems for scientific applications is notoriously convoluted with hundreds of seemingly equivalent workflow systems, many isolated research claims, and a steep learning curve. To address some of these challenges and lay the groundwork for transforming workflows research and development, the WorkflowsRI and ExaWorks projects partnered to bring the international workflows community together. This paper reports on discussions and findings from two virtual "Workflows Community Summits" (January and April, 2021). The overarching goals of these workshops were to develop a view of the state of the art, identify crucial research challenges in the workflows community, articulate a vision for potential community efforts, and discuss technical approaches for realizing this vision. To this end, participants identified six broad themes: FAIR computational workflows; AI workflows; exascale challenges; APIs, interoperability, reuse, and standards; training and education; and building a workflows community. We summarize discussions and recommendations for each of these themes.},
	urldate = {2022-06-28},
	booktitle = {2021 {IEEE} {Workshop} on {Workflows} in {Support} of {Large}-{Scale} {Science} ({WORKS})},
	publisher = {IEEE},
	author = {Ferreira da Silva, Rafael and Casanova, Henri and Chard, Kyle and Altintas, Ilkay and Badia, Rosa M and Balis, Bartosz and Coleman, Taina and Coppens, Frederik and Di Natale, Frank and Enders, Bjoern and Fahringer, Thomas and Filgueira, Rosa and Fursin, Grigori and Garijo, Daniel and Goble, Carole and Howell, Dorran and Jha, Shantenu and Katz, Daniel S. and Laney, Daniel and Leser, Ulf and Malawski, Maciej and Mehta, Kshitij and Pottier, Loic and Ozik, Jonathan and Peterson, J. Luc and Ramakrishnan, Lavanya and Soiland-Reyes, Stian and Thain, Douglas and Wolf, Matthew},
	month = nov,
	year = {2021},
	note = {arXiv:2110.02168 [cs]
interest: 90},
	keywords = {project-acm-rep, workflow managers},
	pages = {81--90},
	file = {arXiv Fulltext PDF:/home/sam/Zotero/storage/FPGAQU2S/da Silva et al. - 2021 - A Community Roadmap for Scientific Workflows Resea.pdf:application/pdf;arXiv.org Snapshot:/home/sam/Zotero/storage/T4DLYZB2/2110.html:text/html},
}

@article{khan_sharing_2019,
	title = {Sharing interoperable workflow provenance: {A} review of best practices and their practical application in {CWLProv}},
	volume = {8},
	issn = {2047-217X},
	shorttitle = {Sharing interoperable workflow provenance},
	url = {https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giz095/5611001},
	doi = {10.1093/gigascience/giz095},
	abstract = {The automation of data analysis in the form of scientific workflows has become a widely adopted practice in many fields of research. Computationally driven data-intensive experiments using workflows enable automation, scaling, adaptation, and provenance support. However, there are still several challenges associated with the effective sharing, publication, and reproducibility of such workflows due to the incomplete capture of provenance and lack of interoperability between different technical (software) platforms.Based on best-practice recommendations identified from the literature on workflow design, sharing, and publishing, we define a hierarchical provenance framework to achieve uniformity in provenance and support comprehensive and fully re-executable workflows equipped with domain-specific information. To realize this framework, we present CWLProv, a standard-based format to represent any workflow-based computational analysis to produce workflow output artefacts that satisfy the various levels of provenance. We use open source community-driven standards, interoperable workflow definitions in Common Workflow Language (CWL), structured provenance representation using the W3C PROV model, and resource aggregation and sharing as workflow-centric research objects generated along with the final outputs of a given workflow enactment. We demonstrate the utility of this approach through a practical implementation of CWLProv and evaluation using real-life genomic workflows developed by independent groups.The underlying principles of the standards utilized by CWLProv enable semantically rich and executable research objects that capture computational workflows with retrospective provenance such that any platform supporting CWL will be able to understand the analysis, reuse the methods for partial reruns, or reproduce the analysis to validate the published findings.},
	language = {en},
	number = {11},
	urldate = {2022-08-02},
	journal = {GigaScience},
	author = {Khan, Farah Zaib and Soiland-Reyes, Stian and Sinnott, Richard O and Lonie, Andrew and Goble, Carole and Crusoe, Michael R},
	month = nov,
	year = {2019},
	note = {interest: 98},
	keywords = {project-acm-rep, provenance, semantic web},
	pages = {giz095},
	file = {Full Text PDF:/home/sam/Zotero/storage/LQPEQUD2/Khan et al. - 2019 - Sharing interoperable workflow provenance A revie.pdf:application/pdf;Snapshot:/home/sam/Zotero/storage/S7NLQ2QK/5611001.html:text/html},
}

@article{hull_taverna_2006,
	title = {Taverna: a tool for building and running workflows of services},
	volume = {34},
	issn = {0305-1048},
	shorttitle = {Taverna},
	url = {https://doi.org/10.1093/nar/gkl320},
	doi = {10.1093/nar/gkl320},
	abstract = {Taverna is an application that eases the use and integration of the growing number of molecular biology tools and databases available on the web, especially web services. It allows bioinformaticians to construct workflows or pipelines of services to perform a range of different analyses, such as sequence analysis and genome annotation. These high-level workflows can integrate many different resources into a single analysis. Taverna is available freely under the terms of the GNU Lesser General Public License (LGPL) from http://taverna.sourceforge.net/ .},
	number = {suppl\_2},
	urldate = {2022-09-06},
	journal = {Nucleic Acids Research},
	author = {Hull, Duncan and Wolstencroft, Katy and Stevens, Robert and Goble, Carole and Pocock, Mathew R. and Li, Peter and Oinn, Tom},
	month = jul,
	year = {2006},
	note = {interest: 89},
	keywords = {reproducibility engineering, workflow managers, project-acm-rep},
	pages = {W729--W732},
	file = {Full Text PDF:/home/sam/Zotero/storage/5YI4EXBV/Hull et al. - 2006 - Taverna a tool for building and running workflows.pdf:application/pdf},
}

@article{hinsen_dealing_2019,
	title = {Dealing {With} {Software} {Collapse}},
	volume = {21},
	issn = {1558-366X},
	doi = {10.1109/MCSE.2019.2900945},
	abstract = {Discusses the concept of software collapse. There is A good chance that you have never heard of software collapse before, for the simple reason that it is a term I have made up myself two years ago in a blog post. However, if you have been doing computational science for a few years, there is a good chance that you have experienced software collapse, and probably it was not a pleasant experience. In this paper, I will explain what software collapse is, what causes it, and how you can manage the risk of it happening to you. What I call software collapse is more commonly referred to as software rot: the fact that software stops working eventually if is not actively maintained. The rot metaphor has a long history, the first documented reference being the 1983 edition of the Hacker’s Dictionary. Back then, it was used jokingly by a small community of computer experts who understood the phenomenon perfectly well, and therefore a funny but technically inaccurate metaphor was not a problem. Today, it is being discussed in much wider circles, for example, in the context of reproducible research. In my opinion, it is appropriate to introduce a useful metaphor in place of the traditional humorous one, because good metaphors contribute to a better understanding of what is actually going on. The main issue with the rot metaphor is that it puts the blame on the wrong piece of the puzzle. If software becomes unusable over time, it is not because of any alteration to that software that needs to be reversed. Rather, it is the foundation on which the software has been built, ranging from the actual hardware via the operating system to programming languages and libraries, that has changed so much that the software is no longer compatible with it. Since unstable foundations resemble how a house is destroyed by an earthquake rather than how spoiling food is transformed by fungi, I consider collapse an appropriate metaphor.},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Hinsen, Konrad},
	month = may,
	year = {2019},
	note = {Conference Name: Computing in Science \& Engineering},
	keywords = {project-acm-rep, research software engineering, software collapse},
	pages = {104--108},
	file = {IEEE Xplore Abstract Record:/home/sam/Zotero/storage/6FX4X7BB/8701540.html:text/html;Submitted Version:/home/sam/Zotero/storage/J9AJJ73B/Hinsen - 2019 - Dealing With Software Collapse.pdf:application/pdf},
}

@article{ewels_nf-core_2020,
	title = {The nf-core framework for community-curated bioinformatics pipelines},
	volume = {38},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-020-0439-x},
	doi = {10.1038/s41587-020-0439-x},
	language = {en},
	number = {3},
	urldate = {2022-10-31},
	journal = {Nat Biotechnol},
	author = {Ewels, Philip A. and Peltzer, Alexander and Fillinger, Sven and Patel, Harshil and Alneberg, Johannes and Wilm, Andreas and Garcia, Maxime Ulysse and Di Tommaso, Paolo and Nahnsen, Sven},
	month = mar,
	year = {2020},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {reproducibility engineering, workflow managers, project-acm-rep},
	pages = {276--278},
	file = {Full Text PDF:/home/sam/Zotero/storage/H4EZXAF5/Ewels et al. - 2020 - The nf-core framework for community-curated bioinf.pdf:application/pdf;Snapshot:/home/sam/Zotero/storage/LQDQZTUV/s41587-020-0439-x.html:text/html},
}

@article{goble_myexperiment_2010,
	title = {{myExperiment}: a repository and social network for the sharing of bioinformatics workflows},
	volume = {38},
	issn = {0305-1048},
	shorttitle = {{myExperiment}},
	url = {https://doi.org/10.1093/nar/gkq429},
	doi = {10.1093/nar/gkq429},
	abstract = {myExperiment (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, myExperiment allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to myExperiment and enable them to be shared in a secure manner. Since its release in 2007, myExperiment currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project. Contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work. Further documentation about myExperiment including its REST web service is available from http://wiki.myexperiment.org. Feedback and requests for support can be sent to bugs@myexperiment.org.},
	number = {suppl\_2},
	urldate = {2022-10-31},
	journal = {Nucleic Acids Research},
	author = {Goble, Carole A. and Bhagat, Jiten and Aleksejevs, Sergejs and Cruickshank, Don and Michaelides, Danius and Newman, David and Borkum, Mark and Bechhofer, Sean and Roos, Marco and Li, Peter and De Roure, David},
	month = jul,
	year = {2010},
	keywords = {reproducibility engineering, workflow managers, project-acm-rep},
	pages = {W677--W682},
	file = {Full Text PDF:/home/sam/Zotero/storage/JWIDGCGY/Goble et al. - 2010 - myExperiment a repository and social network for .pdf:application/pdf},
}

@article{trisovic_large-scale_2022,
	title = {A large-scale study on research code quality and execution},
	volume = {9},
	copyright = {2022 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-022-01143-6},
	doi = {10.1038/s41597-022-01143-6},
	abstract = {This article presents a study on the quality and execution of research code from publicly-available replication datasets at the Harvard Dataverse repository. Research code is typically created by a group of scientists and published together with academic papers to facilitate research transparency and reproducibility. For this study, we define ten questions to address aspects impacting research reproducibility and reuse. First, we retrieve and analyze more than 2000 replication datasets with over 9000 unique R files published from 2010 to 2020. Second, we execute the code in a clean runtime environment to assess its ease of reuse. Common coding errors were identified, and some of them were solved with automatic code cleaning to aid code execution. We find that 74\% of R files failed to complete without error in the initial execution, while 56\% failed when code cleaning was applied, showing that many errors can be prevented with good coding practices. We also analyze the replication datasets from journals’ collections and discuss the impact of the journal policy strictness on the code re-execution rate. Finally, based on our results, we propose a set of recommendations for code dissemination aimed at researchers, journals, and repositories.},
	language = {en},
	number = {1},
	urldate = {2022-12-13},
	journal = {Sci Data},
	author = {Trisovic, Ana and Lau, Matthew K. and Pasquier, Thomas and Crosas, Mercè},
	month = feb,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {reproducibility engineering, project-acm-rep},
	pages = {60},
	file = {Full Text PDF:/home/sam/Zotero/storage/YI4U9WQW/Trisovic et al. - 2022 - A large-scale study on research code quality and e.pdf:application/pdf},
}

@article{stodden_empirical_2018,
	title = {An empirical analysis of journal policy effectiveness for computational reproducibility},
	volume = {115},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1708290115},
	doi = {10.1073/pnas.1708290115},
	abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (i) requesting data and code from authors and (ii) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal Science after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy—author remission of data and code postpublication upon request—an improvement over no policy, but currently insufficient for reproducibility.},
	number = {11},
	urldate = {2023-01-19},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
	month = mar,
	year = {2018},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	keywords = {reproducibility engineering, project-acm-rep},
	pages = {2584--2589},
	file = {Full Text PDF:/home/sam/Zotero/storage/T537TK9E/Stodden et al. - 2018 - An empirical analysis of journal policy effectiven.pdf:application/pdf},
}

@article{wang_retracted_2012,
	title = {Retracted: {Prediction} of posttranslational modification sites from sequences with kernel methods},
	volume = {33},
	issn = {1096-987X},
	shorttitle = {Retracted},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.21526},
	doi = {10.1002/jcc.21526},
	abstract = {The following article from the Journal of Computational Chemistry, “Prediction of Posttranslational Modification Sites from Sequences with Kernel Methods,” by Xiaobo Wang, Yongcui Wang, Yingjie Tian, Xiaojian Shao, Ling-Yun Wu, and Naiyang Deng, published online on 21 April 2010 in Wiley Online Library (wileyonlinelibrary.com), DOI: 10.1002.jcc.21526, has been retracted by agreement between the authors, the journal's editors, and Wiley Periodicals, Inc. The retraction has been agreed because a computational error produced results that led the authors to overstate the level of performance of their computing model.},
	language = {en},
	number = {17},
	urldate = {2023-01-19},
	journal = {Journal of Computational Chemistry},
	author = {Wang, Xiaobo and Wang, Yongcui and Tiang, Yingjie and Shao, Xiaojian and Wu, Ling-Yun and Deng, Naiyang},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jcc.21526},
	keywords = {project-acm-rep, retraction},
	pages = {1524--1524},
	file = {Full Text PDF:/home/sam/Zotero/storage/EVAU4HSD/2012 - Retracted Prediction of posttranslational modific.pdf:application/pdf;Snapshot:/home/sam/Zotero/storage/UXPUFZEJ/jcc.html:text/html},
}

@article{qiu_retraction_2019,
	title = {Retraction {Note}: {Limited} individual attention and online virality of low-quality information},
	volume = {3},
	copyright = {2019 Springer Nature Limited},
	issn = {2397-3374},
	shorttitle = {Retraction {Note}},
	url = {https://www.nature.com/articles/s41562-018-0507-0},
	doi = {10.1038/s41562-018-0507-0},
	abstract = {The authors wish to retract this Letter as follow-up work has highlighted that two errors were committed in the analyses used to produce Figs 4d and 5. In Fig. 4d, a software bug led to an incorrect value of the discriminative power represented by the blue bar. The correct value is τ = 0.17, as opposed to the value τ = 0.15 reported in the Letter. In Fig. 5, the model plot was produced with erroneous data. Produced with the correct data, the authors’ model does not account for the virality of both high- and low-quality information observed in the empirical Facebook data (inset). In the revised figure shown in the correction notice, the distribution of high-quality meme popularity predicted by the model is substantially broader than that of low-quality memes, which do not become popular. Thus, the original conclusion, that the model predicts that low-quality information is just as likely to go viral as high-quality information, is not supported. All other results in the Letter remain valid.},
	language = {en},
	number = {1},
	urldate = {2023-01-19},
	journal = {Nat Hum Behav},
	author = {Qiu, Xiaoyan and Oliveira, Diego F. M. and Shirazi, Alireza Sahami and Flammini, Alessandro and Menczer, Filippo},
	month = jan,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {project-acm-rep, retraction},
	pages = {102--102},
	file = {Full Text PDF:/home/sam/Zotero/storage/9JL4ZV7W/Qiu et al. - 2019 - Retraction Note Limited individual attention and .pdf:application/pdf},
}

@article{villanueva_no_2021,
	title = {No evidence of phosphine in the atmosphere of {Venus} from independent analyses},
	volume = {5},
	copyright = {2021 This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply},
	issn = {2397-3366},
	url = {https://www.nature.com/articles/s41550-021-01422-z},
	doi = {10.1038/s41550-021-01422-z},
	language = {en},
	number = {7},
	urldate = {2023-01-19},
	journal = {Nat Astron},
	author = {Villanueva, G. L. and Cordiner, M. and Irwin, P. G. J. and de Pater, I. and Butler, B. and Gurwell, M. and Milam, S. N. and Nixon, C. A. and Luszcz-Cook, S. H. and Wilson, C. F. and Kofman, V. and Liuzzi, G. and Faggi, S. and Fauchez, T. J. and Lippi, M. and Cosentino, R. and Thelen, A. E. and Moullet, A. and Hartogh, P. and Molter, E. M. and Charnley, S. and Arney, G. N. and Mandell, A. M. and Biver, N. and Vandaele, A. C. and de Kleer, K. R. and Kopparapu, R.},
	month = jul,
	year = {2021},
	note = {Number: 7
Publisher: Nature Publishing Group},
	keywords = {astronomical observations, project-acm-rep, retraction},
	pages = {631--635},
	file = {Full Text PDF:/home/sam/Zotero/storage/3JWNRC2A/Villanueva et al. - 2021 - No evidence of phosphine in the atmosphere of Venu.pdf:application/pdf},
}

@misc{acm_inc_staff_artifact_2020,
	title = {Artifact {Review} and {Badging}},
	url = {https://www.acm.org/publications/policies/artifact-review-and-badging-current},
	abstract = {Result and Artifact Review documentation and badges - V.1.1},
	language = {en},
	urldate = {2023-01-19},
	author = {ACM Inc. staff},
	month = aug,
	year = {2020},
	keywords = {reproducibility, project-acm-rep},
	file = {Snapshot:/home/sam/Zotero/storage/PGDE3ZAE/artifact-review-and-badging-current.html:text/html},
}

@article{stodden_best_2014,
	title = {Best {Practices} for {Computational} {Science}: {Software} {Infrastructure} and {Environments} for {Reproducible} and {Extensible} {Research}},
	volume = {2},
	copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
	issn = {2049-9647},
	shorttitle = {Best {Practices} for {Computational} {Science}},
	url = {http://openresearchsoftware.metajnl.com/articles/10.5334/jors.ay/},
	doi = {10.5334/jors.ay},
	abstract = {The goal of this article is to coalesce a discussion around best practices for scholarly research that utilizes computational methods, by providing a formalized set of best practice recommendations to guide computational scientists and other stakeholders wishing to disseminate reproducible research, facilitate innovation by enabling data and code re-use, and enable broader communication of the output of computational scientific research. Scholarly dissemination and communication standards are changing to reflect the increasingly computational nature of scholarly research, primarily to include the sharing of the data and code associated with published results. We also present these Best Practices as a living, evolving, and changing document at http://wiki.stodden.net/Best\_Practices.},
	language = {en},
	number = {1},
	urldate = {2023-01-19},
	journal = {Journal of Open Research Software},
	author = {Stodden, Victoria and Miguez, Sheila},
	month = jul,
	year = {2014},
	note = {Number: 1
Publisher: Ubiquity Press},
	keywords = {research software engineering, reproducibility engineering, project-acm-rep},
	pages = {e21},
	file = {Full Text PDF:/home/sam/Zotero/storage/5YM2UMKA/Stodden and Miguez - 2014 - Best Practices for Computational Science Software.pdf:application/pdf},
}

@inproceedings{elsner_empirically_2021,
	address = {New York, NY, USA},
	series = {{ISSTA} 2021},
	title = {Empirically evaluating readily available information for regression test optimization in continuous integration},
	isbn = {978-1-4503-8459-9},
	url = {https://doi.org/10.1145/3460319.3464834},
	doi = {10.1145/3460319.3464834},
	abstract = {Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90\% of the failures, we show that practitioners can expect to save on average 84\% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 30th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
	month = jul,
	year = {2021},
	note = {interest: 99},
	keywords = {continuous integration, project-acm-rep, regression testing},
	pages = {491--504},
	file = {Full Text PDF:/home/sam/Zotero/storage/CG4ZZ7MN/Elsner et al. - 2021 - Empirically evaluating readily available informati.pdf:application/pdf},
}

@article{koster_snakemakescalable_2012,
	title = {Snakemake—a scalable bioinformatics workflow engine},
	volume = {28},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/bts480},
	doi = {10.1093/bioinformatics/bts480},
	abstract = {Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.Availability:  http://snakemake.googlecode.com.Contact:  johannes.koester@uni-due.de},
	number = {19},
	urldate = {2023-01-29},
	journal = {Bioinformatics},
	author = {Köster, Johannes and Rahmann, Sven},
	month = oct,
	year = {2012},
	keywords = {research software engineering, reproducibility engineering, workflow managers, project-acm-rep},
	pages = {2520--2522},
	file = {Full Text PDF:/home/sam/Zotero/storage/C7B6FH3N/Köster and Rahmann - 2012 - Snakemake—a scalable bioinformatics workflow engin.pdf:application/pdf},
}

@article{kurtzer_singularity_2017,
	title = {Singularity: {Scientific} containers for mobility of compute},
	volume = {12},
	issn = {1932-6203},
	shorttitle = {Singularity},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459},
	doi = {10.1371/journal.pone.0177459},
	abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
	language = {en},
	number = {5},
	urldate = {2023-01-29},
	journal = {PLOS ONE},
	author = {Kurtzer, Gregory M. and Sochat, Vanessa and Bauer, Michael W.},
	month = may,
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {research software engineering, reproducibility engineering, project-acm-rep},
	pages = {e0177459},
	file = {Full Text PDF:/home/sam/Zotero/storage/MYW2MIFY/Kurtzer et al. - 2017 - Singularity Scientific containers for mobility of.pdf:application/pdf},
}

@misc{holden_increasing_2013,
	title = {Increasing {Access} to the {Results} of {Federally} {Funded} {Scientific} {Research}},
	publisher = {Executive Office of the President, Office of Science and Technology Policy},
	author = {Holden, John P.},
	month = feb,
	year = {2013},
	keywords = {open data, project-acm-rep},
	file = {Holden - 2013 - Increasing Access to the Results of Federally Fund.pdf:/home/sam/Zotero/storage/MBP57769/Holden - 2013 - Increasing Access to the Results of Federally Fund.pdf:application/pdf},
}

@misc{nelson_ensuring_2022,
	title = {Ensuring {Free}, {Immediate}, and {Equitable} {Access} to {Federally} {Funded} {Research}},
	publisher = {Executive Office of the President, Office of Science and Technology Policy},
	author = {Nelson, Alondra},
	month = aug,
	year = {2022},
	keywords = {open data, project-acm-rep},
	file = {Nelson - 2022 - Ensuring Free, Immediate, and Equitable Access to .pdf:/home/sam/Zotero/storage/EYGNDNRM/Nelson - 2022 - Ensuring Free, Immediate, and Equitable Access to .pdf:application/pdf},
}

@misc{zurbuchen_smd_2022,
	title = {{SMD} {Policy} {Document} {SPD}-41a},
	publisher = {NASA Scientific Information Policy for the Science Mission Directorate},
	author = {Zurbuchen, Thomas H.},
	month = sep,
	year = {2022},
	keywords = {opensource software, open data, project-acm-rep},
	file = {Zurbuchen - 2022 - SMD Policy Document SPD-41a.pdf:/home/sam/Zotero/storage/FI2HJWSE/Zurbuchen - 2022 - SMD Policy Document SPD-41a.pdf:application/pdf},
}

@misc{ryan_foundations_2019,
	title = {Foundations for {Evidence}-{Based} {Policymaking} {Act} of 2018},
	author = {Ryan, Paul D.},
	month = jan,
	year = {2019},
	keywords = {project-acm-rep},
}

@article{annan_editorial_2013,
	title = {Editorial: {The} publication of geoscientific model developments v1.0},
	volume = {6},
	issn = {1991-959X},
	shorttitle = {Editorial},
	url = {https://gmd.copernicus.org/articles/6/1233/2013/},
	doi = {10.5194/gmd-6-1233-2013},
	abstract = {{\textless}p{\textgreater}{\textless}strong class="journal-contentHeaderColor"{\textgreater}Abstract.{\textless}/strong{\textgreater} In 2008, the first volume of the European Geosciences Union (EGU) journal Geoscientific Model Development (GMD) was published. GMD was founded because we perceived there to be a need for a space to publish comprehensive descriptions of numerical models in the geosciences. The journal is now well established, with the submission rate increasing over time. However, there are several aspects of model publication that we believe could be further improved. In this editorial we assess the lessons learned over the first few years of the journal's life, and describe some changes to GMD's editorial policy, which will ensure that the models and model developments are published in such a way that they are of maximum value to the community. {\textless}br{\textgreater}{\textless}br{\textgreater} These changes to editorial policy mostly focus on improving the rigour of the review process through a stricter requirement for access to the materials necessary to test the behaviour of the models. {\textless}br{\textgreater}{\textless}br{\textgreater} Throughout this editorial, "must" means that the stated actions are required, and the paper cannot be published without them; "strongly encouraged" means that we encourage the action, but papers can still be published if the criteria are not met; "may" means that the action may be carried out by the authors or referees, if they so wish. {\textless}br{\textgreater}{\textless}br{\textgreater} We have reviewed and rationalised the manuscript types into five new categories. For all papers which are primarily based on a specific numerical model, the changes are as follows:
- The paper must be accompanied by the code, or means of accessing the code, for the purpose of peer-review. If the code is normally distributed in a way which could compromise the anonymity of the referees, then the code must be made available to the editor. The referee/editor is not required to review the code in any way, but they may do so if they so wish.
- All papers must include a section at the end of the paper entitled "Code availability". In this section, instructions for obtaining the code (e.g. from a supplement, or from a website) should be included; alternatively, contact information should be given where the code can be obtained on request, or the reasons why the code is not available should be clearly stated.
- We strongly encourage authors to upload any user manuals associated with the code.
- For models where this is practicable, we strongly encourage referees to compile the code, and run test cases supplied by the authors where appropriate.For models which have been previously described in the "grey" literature (e.g. as internal institutional documents), we strongly encourage authors to include this grey literature as a supplement, when this is allowed by the original authors. 
- All papers must include a model name and version number (or other unique identifier) in the title. 
-It is our perception that, since Geoscientific Model Development (GMD) was founded, it has become increasingly common to see model descriptions published in other more traditional journals, so we hope that our insights may be of general value to the wider geoscientific community.},
	language = {English},
	number = {4},
	urldate = {2023-01-30},
	journal = {Geoscientific Model Development},
	author = {Annan, J. and Hargreaves, D. and Lunt, D. and Ridgwell, A. and Rutt, I. and Sander, R.},
	month = aug,
	year = {2013},
	note = {Publisher: Copernicus GmbH},
	keywords = {reproducibility engineering, open data, project-acm-rep},
	pages = {1233--1242},
	file = {Full Text PDF:/home/sam/Zotero/storage/AHL95BFI/GMD Executive Editors - 2013 - Editorial The publication of geoscientific model .pdf:application/pdf},
}

@article{di_tommaso_nextflow_2017,
	title = {Nextflow enables reproducible computational workflows},
	volume = {35},
	copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/nbt.3820%7B},
	doi = {10.1038/nbt.3820},
	language = {en},
	number = {4},
	urldate = {2023-01-30},
	journal = {Nat Biotechnol},
	author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
	month = apr,
	year = {2017},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {research software engineering, reproducibility engineering, workflow managers, project-acm-rep},
	pages = {316--319},
	file = {Full Text PDF:/home/sam/Zotero/storage/C4D95KCA/Di Tommaso et al. - 2017 - Nextflow enables reproducible computational workfl.pdf:application/pdf},
}

@incollection{brown_case_2007,
	address = {London},
	title = {A {Case} {Study} on the {Use} of {Workflow} {Technologies} for {Scientific} {Analysis}: {Gravitational} {Wave} {Data} {Analysis}},
	isbn = {978-1-84628-757-2},
	shorttitle = {A {Case} {Study} on the {Use} of {Workflow} {Technologies} for {Scientific} {Analysis}},
	url = {https://doi.org/10.1007/978-1-84628-757-2_4},
	abstract = {Modern scientific experiments acquire large amounts of data that must be analyzed in subtle and complicated ways to extract the best results. The Laser Interferometer Gravitational Wave Observatory (LIGO) is an ambitious effort to detect gravitational waves produced by violent events in the universe, such as the collision of two black holes or the explosion of supernovae [37,258]. The experiment records approximately 1 TB of data per day, which is analyzed by scientists in a collaboration that spans four continents. LIGO and distributed computing have grown up side by side over the past decade, and the analysis strategies adopted by LIGO scientists have been strongly influenced by the increasing power of tools to manage distributed computing resources and the workflows to run on them. In this chapter, we use LIGO as an application case study in workflow design and implementation. The software architecture outlined here has been used with great efficacy to analyze LIGO data [2–5] using dedicated computing facilities operated by the LIGO Scientific Collaboration, the LIGO Data Grid. It is just the first step, however. Workflow design and implementation lies at the interface between computing and traditional scientific activities. In the conclusion, we outline a few directions for future development and provide some long-term vision for applications related to gravitational wave data analysis.},
	language = {en},
	urldate = {2023-01-31},
	booktitle = {Workflows for e-{Science}: {Scientific} {Workflows} for {Grids}},
	publisher = {Springer},
	author = {Brown, Duncan A. and Brady, Patrick R. and Dietz, Alexander and Cao, Junwei and Johnson, Ben and McNabb, John},
	editor = {Taylor, Ian J. and Deelman, Ewa and Gannon, Dennis B. and Shields, Matthew},
	year = {2007},
	doi = {10.1007/978-1-84628-757-2_4},
	keywords = {workflow managers, project-acm-rep},
	pages = {39--59},
}

@inproceedings{rougier_rescience_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ReScience} {C}: {A} {Journal} for {Reproducible} {Replications} in {Computational} {Science}},
	isbn = {978-3-030-23987-9},
	shorttitle = {{ReScience} {C}},
	doi = {10.1007/978-3-030-23987-9_14},
	abstract = {Independent replication is one of the most powerful methods to verify published scientific studies. In computational science, it requires the reimplementation of the methods described in the original article by a different team of researchers. Replication is often performed by scientists who wish to gain a better understanding of a published method, but its results are rarely made public. ReScience C is a peer-reviewed journal dedicated to the publication of high-quality computational replications that provide added value to the scientific community. To this end, ReScience C requires replications to be reproducible and implemented using Open Source languages and libraries. In this article, we provide an overview of ReScience C’s goals and quality standards, outline the submission and reviewing processes, and summarize the experience of its first three years of operation, concluding with an outlook towards evolutions envisaged for the near future.},
	language = {en},
	booktitle = {Reproducible {Research} in {Pattern} {Recognition}},
	publisher = {Springer International Publishing},
	author = {Rougier, Nicolas P. and Hinsen, Konrad},
	editor = {Kerautret, Bertrand and Colom, Miguel and Lopresti, Daniel and Monasse, Pascal and Talbot, Hugues},
	year = {2019},
	keywords = {reproducibility engineering, project-acm-rep},
	pages = {150--156},
	file = {Submitted Version:/home/sam/Zotero/storage/3YZGFA62/Rougier and Hinsen - 2019 - ReScience C A Journal for Reproducible Replicatio.pdf:application/pdf},
}

@article{krafczyk_learning_2021,
	title = {Learning from reproducing computational results: introducing three principles and the {Reproduction} {Package}},
	volume = {379},
	shorttitle = {Learning from reproducing computational results},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0069},
	doi = {10.1098/rsta.2020.0069},
	abstract = {We carry out efforts to reproduce computational results for seven published articles and identify barriers to computational reproducibility. We then derive three principles to guide the practice and dissemination of reproducible computational research: (i) Provide transparency regarding how computational results are produced; (ii) When writing and releasing research software, aim for ease of (re-)executability; (iii) Make any code upon which the results rely as deterministic as possible. We then exemplify these three principles with 12 specific guidelines for their implementation in practice. We illustrate the three principles of reproducible research with a series of vignettes from our experimental reproducibility work. We define a novel Reproduction Package, a formalism that specifies a structured way to share computational research artifacts that implements the guidelines generated from our reproduction efforts to allow others to build, reproduce and extend computational science. We make our reproduction efforts in this paper publicly available as exemplar Reproduction Packages.

This article is part of the theme issue ‘Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico’.},
	number = {2197},
	urldate = {2023-01-31},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Krafczyk, M. S. and Shi, A. and Bhaskar, A. and Marinov, D. and Stodden, V.},
	month = mar,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {reproducibility engineering, project-acm-rep},
	pages = {20200069},
	file = {Full Text PDF:/home/sam/Zotero/storage/P3RYMMMA/Krafczyk et al. - 2021 - Learning from reproducing computational results i.pdf:application/pdf},
}

@inproceedings{folk_overview_2011,
	address = {New York, NY, USA},
	series = {{AD} '11},
	title = {An overview of the {HDF5} technology suite and its applications},
	isbn = {978-1-4503-0614-0},
	url = {https://doi.org/10.1145/1966895.1966900},
	doi = {10.1145/1966895.1966900},
	abstract = {In this paper, we give an overview of the HDF5 technology suite and some of its applications. We discuss the HDF5 data model, the HDF5 software architecture and some of its performance enhancing capabilities.},
	urldate = {2023-02-06},
	booktitle = {Proceedings of the {EDBT}/{ICDT} 2011 {Workshop} on {Array} {Databases}},
	publisher = {Association for Computing Machinery},
	author = {Folk, Mike and Heber, Gerd and Koziol, Quincey and Pourmal, Elena and Robinson, Dana},
	month = mar,
	year = {2011},
	keywords = {research software engineering, databases, project-acm-rep},
	pages = {36--47},
	file = {Full Text PDF:/home/sam/Zotero/storage/RPGSE2L5/Folk et al. - 2011 - An overview of the HDF5 technology suite and its a.pdf:application/pdf},
}

@techreport{ferreira_da_silva_workflows_2021,
	title = {Workflows {Community} {Summit}: {Tightening} the {Integration} between {Computing} {Facilities} and {Scientific} {Workflows}},
	shorttitle = {Workflows {Community} {Summit}},
	url = {https://www.osti.gov/biblio/1842590},
	abstract = {Scientific workflows are used almost universally across science domains for solving complex and largescale computing and data analysis problems. The importance of workflows is highlighted by the fact that they have underpinned some of the most significant discoveries of the past decades. Many of these workflows have significant computational, storage, and communication demands, and thus must execute on a range of large-scale computer systems, from local clusters to public clouds and upcoming exascale HPC platforms. Managing these executions is often a significant undertaking, requiring a sophisticated and versatile software infrastructure. Historically, infrastructures for workflow execution consisted of complex, integrated systems, developed in-house by workflow practitioners with strong dependencies on a range of legacy technologies—even including sets of ad hoc scripts. Due to the increasing need to support workflows, dedicated workflow systems were developed to provide abstractions for creating, executing, and adapting workflows conveniently and efficiently while ensuring portability. While these efforts are all worthwhile individually, there are now hundreds of independent workflow systems. These workflow systems are created and used by thousands of researchers and developers, leading to a rapidly growing corpus of workflows research publications. The resulting workflow system technology landscape is fragmented, which may present significant barriers for future workflow users due to many seemingly comparable, yet usually mutually incompatible, systems that exist. In order to tackle some of the challenges described above, the DOE-funded ExaWorks and NSF-funded WorkflowsRI projects have organized in 2021 a series of events entitled the “Workflows Community Summit”. The third edition of the “Workflows Community Summit” explored workflows challenges and opportunities from the perspective of computing centers and facilities. This third summit builds on two prior summits (https://workflowsri.org/summits) that (i) established a high level vision for workflows research; and (ii) explored technical approaches for realizing that vision. The third summit brought together a small group of facilities representatives with the aim to understand how workflows are currently being used at each facility, how facilities would like to interact with workflow developers and users, how workflows fit with facility roadmaps, and what opportunities there are for tighter integration between facilities and workflows. This report documents and organizes the wealth of information provided by the participants before, during, and after the summit.},
	language = {English},
	number = {ORNL/TM-2022/1832},
	urldate = {2023-02-16},
	institution = {Oak Ridge National Lab. (ORNL), Oak Ridge, TN (United States)},
	author = {Ferreira da Silva, Rafael and Chard, Kyle and Casanova, Henri and Laney, Daniel and Ahn, Dong H. and Jha, Shantenu and Allcock, William E. and Bauer, Gregory and Duplyakin, Dmitry and Enders, Bjoern and Heer, Todd M. and Lancon, Eric and Sanielevici, Sergiu and Sayers, Kevin},
	month = nov,
	year = {2021},
	doi = {10.2172/1842590},
	keywords = {project-acm-rep, research software engineering, workflow managers},
	file = {Full Text PDF:/home/sam/Zotero/storage/4L5KHWV6/Ferreira da Silva et al. - 2021 - Workflows Community Summit Tightening the Integra.pdf:application/pdf},
}

@inproceedings{gomez-perez_when_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {When {History} {Matters} - {Assessing} {Reliability} for the {Reuse} of {Scientific} {Workflows}},
	isbn = {978-3-642-41338-4},
	doi = {10.1007/978-3-642-41338-4_6},
	abstract = {Scientific workflows play an important role in computational research as essential artifacts for communicating the methods used to produce research findings. We are witnessing a growing number of efforts that treat workflows as first-class artifacts for sharing and exchanging scientific knowledge, either as part of scholarly articles or as stand-alone objects. However, workflows are not born to be reliable, which can seriously damage their reusability and trustworthiness as knowledge exchange instruments. Scientific workflows are commonly subject to decay, which consequently undermines their reliability over their lifetime. The reliability of workflows can be notably improved by advocating scientists to preserve a minimal set of information that is essential to assist the interpretations of these workflows and hence improve their potential for reproducibility and reusability. In this paper we show how, by measuring and monitoring the completeness and stability of scientific workflows over time we are able to provide scientists with a measure of their reliability, supporting the reuse of trustworthy scientific knowledge.},
	language = {en},
	booktitle = {The {Semantic} {Web} – {ISWC} 2013},
	publisher = {Springer},
	author = {Gómez-Pérez, José Manuel and García-Cuesta, Esteban and Garrido, Aleix and Ruiz, José Enrique and Zhao, Jun and Klyne, Graham},
	editor = {Alani, Harith and Kagal, Lalana and Fokoue, Achille and Groth, Paul and Biemann, Chris and Parreira, Josiane Xavier and Aroyo, Lora and Noy, Natasha and Welty, Chris and Janowicz, Krzysztof},
	year = {2013},
	keywords = {project-acm-rep, reproducibility, workflow managers},
	pages = {81--97},
	file = {Full Text PDF:/home/sam/Zotero/storage/5ARRBNBG/Gómez-Pérez et al. - 2013 - When History Matters - Assessing Reliability for t.pdf:application/pdf},
}

@article{deelman_future_2018,
	title = {The future of scientific workflows},
	volume = {32},
	issn = {1094-3420},
	url = {https://doi.org/10.1177/1094342017704893},
	doi = {10.1177/1094342017704893},
	abstract = {Today's computational, experimental, and observational sciences rely on computations that involve many related tasks. The success of a scientific mission often hinges on the computer automation of these workflows. In April 2015, the US Department of Energy (DOE) invited a diverse group of domain and computer scientists from national laboratories supported by the Office of Science, the National Nuclear Security Administration, from industry, and from academia to review the workflow requirements of DOE?s science and national security missions, to assess the current state of the art in science workflows, to understand the impact of emerging extreme-scale computing systems on those workflows, and to develop requirements for automated workflow management in future and existing environments. This article is a summary of the opinions of over 50 leading researchers attending this workshop. We highlight use cases, computing systems, workflow needs and conclude by summarizing the remaining challenges this community sees that inhibit large-scale scientific workflows from becoming a mainstream tool for extreme-scale science.},
	language = {en},
	number = {1},
	urldate = {2023-02-16},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Deelman, Ewa and Peterka, Tom and Altintas, Ilkay and Carothers, Christopher D and van Dam, Kerstin Kleese and Moreland, Kenneth and Parashar, Manish and Ramakrishnan, Lavanya and Taufer, Michela and Vetter, Jeffrey},
	month = jan,
	year = {2018},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {project-acm-rep, workflow managers},
	pages = {159--175},
	file = {SAGE PDF Full Text:/home/sam/Zotero/storage/UTFN4KSI/Deelman et al. - 2018 - The future of scientific workflows.pdf:application/pdf},
}

@inproceedings{butt_provone_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ProvONE}+: {A} {Provenance} {Model} for {Scientific} {Workflows}},
	isbn = {978-3-030-62008-0},
	shorttitle = {{ProvONE}+},
	doi = {10.1007/978-3-030-62008-0_30},
	abstract = {The provenance of workflows is essential, both for the data they derive and for their specification, to allow for the reproducibility, sharing and reuse of information in the scientific community. Although the formal modelling of scientific workflow provenance was of interest and studied, in many fields like semantic web, yet no provenance model has existed, we are aware of, to model control-flow driven scientific workflows. The provenance models proposed by the semantic web community for data-driven scientific workflows may capture the provenance of control-flow driven workflows execution traces (i.e., retrospective provenance) but underspecify the workflow structure (i.e., workflow provenance). An underspecified or incomplete structure of a workflow results in the misinterpretation of a scientific experiment and precludes conformance checking of the workflow, thereby restricting the gains of provenance. To overcome the limitation, we present a formal, lightweight and general-purpose specification model for the control-flows involved scientific workflows. The proposed model can be combined with the existing provenance models and easy to extend to specify the common control-flow patterns. In this article, we inspire the need for control-flow driven scientific workflow provenance model, provide an overview of its key classes and properties, and briefly discuss its integration with the ProvONE provenance model as well as its compatibility to PROV-DM. We will also focus on the sample modelling using the proposed model and present a comprehensive implementation scenario from the agricultural domain for validating the model.},
	language = {en},
	booktitle = {Web {Information} {Systems} {Engineering} – {WISE} 2020},
	publisher = {Springer International Publishing},
	author = {Butt, Anila Sahar and Fitch, Peter},
	editor = {Huang, Zhisheng and Beek, Wouter and Wang, Hua and Zhou, Rui and Zhang, Yanchun},
	year = {2020},
	keywords = {project-acm-rep, provenance, semantic web},
	pages = {431--444},
	file = {Full Text PDF:/home/sam/Zotero/storage/XIV45RJ9/Butt and Fitch - 2020 - ProvONE+ A Provenance Model for Scientific Workfl.pdf:application/pdf},
}

@misc{dykstra_apptainer_2022,
	title = {Apptainer {Without} {Setuid}},
	url = {http://arxiv.org/abs/2208.12106},
	doi = {10.48550/arXiv.2208.12106},
	abstract = {Apptainer (formerly known as Singularity) since its beginning implemented many of its container features with the assistance of a setuid-root program. It still supports that mode, but as of version 1.1.0 it no longer uses setuid by default. This is feasible because it now can mount squash filesystems, mount ext2/3/4 filesystems, and use overlayfs using unprivileged user namespaces and FUSE. It also now enables unprivileged users to build containers, even without requiring system administrators to configure /etc/subuid and /etc/subgid unlike other "rootless" container systems. As a result, all the unprivileged functions can be used nested inside of another container, even if the container runtime prevents any elevated privileges.},
	urldate = {2023-02-18},
	publisher = {arXiv},
	author = {Dykstra, Dave},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12106 [cs]},
	keywords = {containers, operating systems, project-acm-rep, reproducibility engineering},
	file = {arXiv Fulltext PDF:/home/sam/Zotero/storage/H2LSHPL3/Dykstra - 2022 - Apptainer Without Setuid.pdf:application/pdf;arXiv.org Snapshot:/home/sam/Zotero/storage/AME9T7TQ/2208.html:text/html},
}

@misc{singularity_developers_security_2023,
	title = {Security in {SingularityCE} — {SingularityCE} {Admin} {Guide} 3.11 documentation},
	url = {https://docs.sylabs.io/guides/latest/admin-guide/security.html},
	urldate = {2023-02-18},
	author = {Singularity Developers},
	year = {2023},
	keywords = {containers, operating systems, project-acm-rep},
	file = {Security in SingularityCE — SingularityCE Admin Guide 3.11 documentation:/home/sam/Zotero/storage/NBP7QE8A/security.html:text/html},
}

@misc{linux_developers_user_namespaces7_2021,
	title = {user\_namespaces(7) - {Linux} manual page},
	url = {https://www.man7.org/linux/man-pages/man7/user_namespaces.7.html},
	urldate = {2023-02-18},
	author = {Linux Developers},
	month = aug,
	year = {2021},
	keywords = {containers, operating systems, project-acm-rep},
	file = {user_namespaces(7) - Linux manual page:/home/sam/Zotero/storage/NJTIHU8G/user_namespaces.7.html:text/html},
}

@misc{cve_database_cve_2020,
	title = {{CVE} - {CVE}-2020-14386},
	url = {https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14386},
	urldate = {2023-02-18},
	author = {CVE database},
	month = jun,
	year = {2020},
	keywords = {containers, project-acm-rep, security},
	file = {CVE - CVE-2020-14386:/home/sam/Zotero/storage/U2ID5CHS/cvename.html:text/html},
}

@misc{stig_authors_red_2020,
	title = {Red {Hat} {Enterprise} {Linux} 8 {Security} {Technical} {Implementation} {Guide}},
	url = {https://www.stigviewer.com/stig/red_hat_enterprise_linux_8/2020-11-25/},
	abstract = {Security Technical Implementation Guides (STIGs) that provides a methodology for standardized secure installation and maintenance of DOD IA and IA-enabled devices and systems.},
	language = {en-US},
	urldate = {2023-02-18},
	journal = {STIG Viewer {\textbar} Unified Compliance Framework®},
	author = {STIG Authors},
	month = nov,
	year = {2020},
	keywords = {operating systems, project-acm-rep, security},
	file = {Snapshot:/home/sam/Zotero/storage/5MW6IHB4/2020-11-25.html:text/html},
}

@article{katz_transitive_2015,
	title = {Transitive {Credit} and {JSON}-{LD}},
	volume = {3},
	copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
	issn = {2049-9647},
	url = {http://openresearchsoftware.metajnl.com/article/10.5334/jors.by/},
	doi = {10.5334/jors.by},
	abstract = {Article: Transitive Credit and JSON-LD},
	language = {en},
	number = {1},
	urldate = {2023-02-20},
	journal = {Journal of Open Research Software},
	author = {Katz, Daniel S. and Smith, Arfon M.},
	month = nov,
	year = {2015},
	note = {Number: 1
Publisher: Ubiquity Press},
	keywords = {academic publishing, project-acm-rep},
	pages = {e7},
	file = {Full Text PDF:/home/sam/Zotero/storage/VADKNANZ/Katz and Smith - 2015 - Transitive Credit and JSON-LD.pdf:application/pdf},
}

@article{beaulieu-jones_reproducibility_2017,
	title = {Reproducibility of computational workflows is automated using continuous analysis},
	volume = {35},
	copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/nbt.3780},
	doi = {10.1038/nbt.3780},
	abstract = {The application of continuous integration, an approach common in software development, enables the automatic reproduction of computational analyses.},
	language = {en},
	number = {4},
	urldate = {2023-02-20},
	journal = {Nat Biotechnol},
	author = {Beaulieu-Jones, Brett K. and Greene, Casey S.},
	month = apr,
	year = {2017},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {project-acm-rep, reproducibility engineering, workflow managers},
	pages = {342--346},
	file = {Full Text PDF:/home/sam/Zotero/storage/B26RFVP4/Beaulieu-Jones and Greene - 2017 - Reproducibility of computational workflows is auto.pdf:application/pdf},
}

@article{pouchard_computational_2019,
	title = {Computational reproducibility of scientific workflows at extreme scales},
	volume = {33},
	issn = {1094-3420},
	url = {https://www.osti.gov/pages/biblio/1542776},
	doi = {10.1177/1094342019839124},
	abstract = {We propose an approach for improved reproducibility that includes capturing and relating provenance characteristics and performance metrics. We discuss two use cases: scientific reproducibility of results in the Energy Exascale Earth System Model (E3SM – previously ACME), and performance reproducibility in molecular dynamics workflows on HPC computing platforms. In order to capture and persist the provenance and performance data of these workflows, we have designed and developed the Chimbuko and ProvEn frameworks. Chimbuko captures provenance and enables detailed single workflow performance analysis. ProvEn is a hybrid, queriable system for storing and analyzing the provenance and performance metrics of multiple runs in workflow performance analysis campaigns. Workflow provenance and performance data output from Chimbuko can be visualized in a dynamic, multi-level visualization providing overview and zoom-in capabilities for areas of interest. Provenance and related performance data ingested into ProvEn is queriable and can be used to reproduce runs. In conclusion, our provenance-based approach highlights challenges in extracting information and gaps in the information collected. It is agnostic to the type of provenance data it captures so that both the reproducibility of scientific results and that of performance can be explored with our tools.},
	language = {English},
	number = {5},
	urldate = {2023-02-20},
	journal = {International Journal of High Performance Computing Applications},
	author = {Pouchard, Line and Baldwin, Sterling and Elsethagen, Todd and Jha, Shantenu and Raju, Bibi and Stephan, Eric and Tang, Li and Van Dam, Kerstin Kleese},
	month = apr,
	year = {2019},
	note = {Institution: Brookhaven National Lab. (BNL), Upton, NY (United States)
Number: BNL-211854-2019-JAAM
Publisher: SAGE},
	keywords = {project-acm-rep, reproducibility engineering, workflow managers},
	pages = {763--776},
	file = {Full Text PDF:/home/sam/Zotero/storage/EUIUMYIZ/Pouchard et al. - 2019 - Computational reproducibility of scientific workfl.pdf:application/pdf},
}

@article{meng_facilitating_2017,
	series = {International {Conference} on {Computational} {Science}, {ICCS} 2017, 12-14 {June} 2017, {Zurich}, {Switzerland}},
	title = {Facilitating the {Reproducibility} of {Scientific} {Workflows} with {Execution} {Environment} {Specifications}},
	volume = {108},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050917306816},
	doi = {10.1016/j.procs.2017.05.116},
	abstract = {Scientific workflows are designed to solve complex scientific problems and accelerate scientific progress. Ideally, scientific workflows should improve the reproducibility of scientific applications by making it easier to share and reuse workflows between scientists. However, scientists often find it difficult to reuse others’ workflows, which is known as workflow decay. In this paper, we explore the challenges in reproducing scientific workflows, and propose a framework for facilitating the reproducibility of scientific workflows at the task level by giving scientists complete control over the execution environments of the tasks in their workflows and integrating execution environment specifications into scientific workflow systems. Our framework allows dependencies to be archived in basic units of OS image, software and data instead of gigantic all-in-one images. We implement a prototype of our framework by integrating Umbrella, an execution environment creator, into Makeflow, a scientific workflow system. To evaluate our framework, we use it to run two bioinformatics scientific workflows, BLAST and BWA. The execution environment of the tasks in each workflow is specified as an Umbrella specification file, and sent to execution nodes where Umbrella is used to create the specified environment for running the tasks. For each workflow we evaluate the size of the Umbrella specification file, the time and space overheads of creating execution environments using Umbrella, and the heterogeneity of execution nodes contributing to each workflow. The evaluation results show that our framework improves the utilization of heterogeneous computing resources, and improves the portability and reproducibility of scientific workflows.},
	language = {en},
	urldate = {2023-02-20},
	journal = {Procedia Computer Science},
	author = {Meng, Haiyan and Thain, Douglas},
	month = jan,
	year = {2017},
	keywords = {project-acm-rep, reproducibility engineering, workflow managers},
	pages = {705--714},
	file = {ScienceDirect Full Text PDF:/home/sam/Zotero/storage/CJZ2CVK8/Meng and Thain - 2017 - Facilitating the Reproducibility of Scientific Wor.pdf:application/pdf;ScienceDirect Snapshot:/home/sam/Zotero/storage/U866VWV9/S1877050917306816.html:text/html},
}
